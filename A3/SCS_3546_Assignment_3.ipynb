{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aHEi-CRlKBb"
   },
   "source": [
    "# Calculating similarity measures between queries and sample documents  \n",
    "\n",
    "Objectives are to demonestrate: \n",
    "- How to preprocess text and embedd textual data\n",
    "-  Compare the results of textual similarity between tradditional and dep learning based methods   \n",
    "\n",
    "\n",
    "*** Important consideration: You are not expected to use any particular library or any particular method; the codes below are just meant to provide you with some help so you spend most of your time on the deep learning based model. Feel free to choose your own methods. the evaluation is based on being able to obtain results regardless of which method is being used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnvVdcQSiKMA"
   },
   "source": [
    "# Set-up and import data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "XkrMh4W0g4kQ",
    "outputId": "42f55e6b-ded7-4592-dfcb-ef8a7a63ad7c"
   },
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0VlWbMwpg4u5"
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('data/sample_repository.json') as in_file:\n",
    "    test_data = json.load(in_file)\n",
    "\n",
    "titles = [item[0] for item in test_data['data']]\n",
    "documents = [item[1] for item in test_data['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>botany</td>\n",
       "      <td>Botany, also called plant science(s), plant bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ford Bronco</td>\n",
       "      <td>The Ford Bronco is a model line of sport utili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>List of fruit dishes</td>\n",
       "      <td>Fruit dishes are those that use fruit as a pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Neuro linguistic programming</td>\n",
       "      <td>Neuro linguistic programming (NLP) is a pseudo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>fruit serving bowl</td>\n",
       "      <td>A fruit serving bowl is a round dish or contai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          titles  \\\n",
       "27                        botany   \n",
       "28                  Ford Bronco    \n",
       "29          List of fruit dishes   \n",
       "30  Neuro linguistic programming   \n",
       "31            fruit serving bowl   \n",
       "\n",
       "                                            documents  \n",
       "27  Botany, also called plant science(s), plant bi...  \n",
       "28  The Ford Bronco is a model line of sport utili...  \n",
       "29  Fruit dishes are those that use fruit as a pri...  \n",
       "30  Neuro linguistic programming (NLP) is a pseudo...  \n",
       "31  A fruit serving bowl is a round dish or contai...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(list(zip(titles, documents)), columns =['titles', 'documents'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1zBu6DIjdtP"
   },
   "source": [
    "# 1. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XP9_ncYg4yu",
    "outputId": "71eb8310-4ec5-4cce-d466-1069be12ae0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sarthakkaushik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sarthakkaushik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Ym076HWMg42r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomegranate Bhagwa\n",
      "Major Market\n",
      "Pink Onions\n",
      "Pomegranate Arakta\n",
      "About Us\n",
      "Contact Us\n",
      "White Onions\n",
      "Video Gallery\n",
      "Food classes\n",
      "Nutrition\n"
     ]
    }
   ],
   "source": [
    "query = 'vegetables'\n",
    "# query  terms: 'fruits' / 'vegetables' / 'healthy foods in Canada'\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectors = vectorizer.fit_transform([query] + documents)\n",
    "\n",
    "# Calculate the word frequency, and a measure of similarity (whatever you find it to be approperiate) of the search terms with each document\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(df.documents)\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)\n",
    "\n",
    "wordcount_dict = dict(zip(word_list,count_list))\n",
    "\n",
    "'''QUERY'''\n",
    "query_vec = vectorizer.transform([query])\n",
    "\n",
    "results = linear_kernel(vectors,query_vec).reshape((-1,))\n",
    "\n",
    "# Print Top 10 results\n",
    "for i in results.argsort()[-10:][::-1]:\n",
    "    print(df.iloc[i,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqrN27J9mZwb"
   },
   "source": [
    "## Repeat the same task after some preprocessing \n",
    "\n",
    "You are not expected to do any specific type of cleaning/standardizations but at minimum use 2 techniques (e.g. lemmatization, removing punctuations and etc.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/sarthakkaushik/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sarthakkaushik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sarthakkaushik/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def pre_processing(text):\n",
    "    # Remove Punctuation\n",
    "    import re\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    \n",
    "    # Tokenize Words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # \n",
    "    #words = [PorterStemmer().stem(w) for w in words]\n",
    "    words = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extremely'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(\"extremely\", pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extrem'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"extremely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fresh',\n",
       " 'pomegranate',\n",
       " 'anushka',\n",
       " 'avni',\n",
       " 'international',\n",
       " 'bhagwa',\n",
       " 'premium',\n",
       " 'pomegranate',\n",
       " 'variety',\n",
       " 'india',\n",
       " 'deep',\n",
       " 'red',\n",
       " 'arils',\n",
       " 'please',\n",
       " 'red',\n",
       " 'rugged',\n",
       " 'skin',\n",
       " 'enhance',\n",
       " 'appearance',\n",
       " 'whilst',\n",
       " 'promote',\n",
       " 'shelf',\n",
       " 'life',\n",
       " 'fruit',\n",
       " 'bhagwa',\n",
       " 'widely',\n",
       " 'know',\n",
       " 'soft',\n",
       " 'seed',\n",
       " 'dark',\n",
       " 'red',\n",
       " 'color',\n",
       " 'extremely',\n",
       " 'delicious',\n",
       " 'package',\n",
       " 'net',\n",
       " 'weight',\n",
       " 'box',\n",
       " '2',\n",
       " '5kg',\n",
       " '3',\n",
       " '00kg',\n",
       " '3',\n",
       " '5kg',\n",
       " 'detail',\n",
       " 'minimum',\n",
       " 'weight',\n",
       " '180gm',\n",
       " 'maximum',\n",
       " 'weight',\n",
       " '400gm',\n",
       " 'color',\n",
       " 'arils',\n",
       " 'dark',\n",
       " 'cherry',\n",
       " 'red',\n",
       " 'taste',\n",
       " 'sweet',\n",
       " 'fruit',\n",
       " 'count',\n",
       " 'carton',\n",
       " '3',\n",
       " '50',\n",
       " 'kg',\n",
       " 'net',\n",
       " 'wt',\n",
       " '9',\n",
       " 'number',\n",
       " 'pack',\n",
       " 'per',\n",
       " 'carton',\n",
       " '350',\n",
       " '400gms']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processing(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QSsdfQb9g46z"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>documents</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>documents_stop_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pomegranate Bhagwa</td>\n",
       "      <td>Fresh Pomegranate from Anushka Avni Internatio...</td>\n",
       "      <td>[fresh, pomegranate, from, anushka, avni, inte...</td>\n",
       "      <td>[fresh, pomegranate, anushka, avni, internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pomegranate Arakta</td>\n",
       "      <td>Fresh Pomegranate Arakta from Anushka Avni Int...</td>\n",
       "      <td>[fresh, pomegranate, arakta, from, anushka, av...</td>\n",
       "      <td>[fresh, pomegranate, arakta, anushka, avni, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>About Us</td>\n",
       "      <td>About Us Anushka Avni International (AAI) take...</td>\n",
       "      <td>[about, us, anushka, avni, international, (, a...</td>\n",
       "      <td>[us, anushka, avni, international, (, aai, ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contact Us</td>\n",
       "      <td>About Us Anushka Avni International (AAI) take...</td>\n",
       "      <td>[about, us, anushka, avni, international, (, a...</td>\n",
       "      <td>[us, anushka, avni, international, (, aai, ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White Onions</td>\n",
       "      <td>White Onions from Anushka Avni International F...</td>\n",
       "      <td>[white, onions, from, anushka, avni, internati...</td>\n",
       "      <td>[white, onions, anushka, avni, international, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               titles                                          documents  \\\n",
       "0  Pomegranate Bhagwa  Fresh Pomegranate from Anushka Avni Internatio...   \n",
       "1  Pomegranate Arakta  Fresh Pomegranate Arakta from Anushka Avni Int...   \n",
       "2            About Us  About Us Anushka Avni International (AAI) take...   \n",
       "3          Contact Us  About Us Anushka Avni International (AAI) take...   \n",
       "4        White Onions  White Onions from Anushka Avni International F...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [fresh, pomegranate, from, anushka, avni, inte...   \n",
       "1  [fresh, pomegranate, arakta, from, anushka, av...   \n",
       "2  [about, us, anushka, avni, international, (, a...   \n",
       "3  [about, us, anushka, avni, international, (, a...   \n",
       "4  [white, onions, from, anushka, avni, internati...   \n",
       "\n",
       "                                  documents_stop_rem  \n",
       "0  [fresh, pomegranate, anushka, avni, internatio...  \n",
       "1  [fresh, pomegranate, arakta, anushka, avni, in...  \n",
       "2  [us, anushka, avni, international, (, aai, ), ...  \n",
       "3  [us, anushka, avni, international, (, aai, ), ...  \n",
       "4  [white, onions, anushka, avni, international, ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can clean, standar a lemmatizer\n",
    "# To reduces words down to their simplest 'lemma' (e.g. helpful when dealing with plurals) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh Pomegranate Arakta from Anushka Avni International This Pomegranate are bigger in size, sweet with soft seeds, bold red arils. It also possess glossy, attractive, dark red skin. Packaging: Net weight of box 2.5kg, 3.00kg, 3.5kg. Details: Minimum Weight 180gm, maximum weight 400gm Taste: Sweet Fruit count / carton (3.50 kg net wt.) 9 Numbers packed per carton: 350-400gms 10 Numbers packed per carton :290-320gms 12 Numbers packed per carton: 275-325gms 15 Numbers packed per carton: 225-275gms Load ability: 4400 cartons per container 20 pallets with 220 cartons per pallet Load ability: 5500 cartons per container Loading with No pallets Availability: January | February | March | April | July | August | September | October | Nov | Dec READ MORE\n"
     ]
    }
   ],
   "source": [
    "print(df.documents.iloc[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fresh', 'Pomegranate', 'Arakta', 'Anushka', 'Avni', 'International', 'This', 'Pomegranate', 'bigger', 'size', ',', 'sweet', 'soft', 'seeds', ',', 'bold', 'red', 'arils', '.', 'It', 'also', 'possess', 'glossy', ',', 'attractive', ',', 'dark', 'red', 'skin', '.', 'Packaging', ':', 'Net', 'weight', 'box', '2.5kg', ',', '3.00kg', ',', '3.5kg', '.', 'Details', ':', 'Minimum', 'Weight', '180gm', ',', 'maximum', 'weight', '400gm', 'Taste', ':', 'Sweet', 'Fruit', 'count', '/', 'carton', '(', '3.50', 'kg', 'net', 'wt', '.', ')', '9', 'Numbers', 'packed', 'per', 'carton', ':', '350-400gms', '10', 'Numbers', 'packed', 'per', 'carton', ':290-320gms', '12', 'Numbers', 'packed', 'per', 'carton', ':', '275-325gms', '15', 'Numbers', 'packed', 'per', 'carton', ':', '225-275gms', 'Load', 'ability', ':', '4400', 'cartons', 'per', 'container', '20', 'pallets', '220', 'cartons', 'per', 'pallet', 'Load', 'ability', ':', '5500', 'cartons', 'per', 'container', 'Loading', 'No', 'pallets', 'Availability', ':', 'January', '|', 'February', '|', 'March', '|', 'April', '|', 'July', '|', 'August', '|', 'September', '|', 'October', '|', 'Nov', '|', 'Dec', 'READ', 'MORE']\n"
     ]
    }
   ],
   "source": [
    "print(df.documents_stop_rem.iloc[1,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaHFkf8tnuJB"
   },
   "outputs": [],
   "source": [
    "#!pip install tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8V2kx6Epnxq"
   },
   "source": [
    "# 2. Semantic matching using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGBg_Roo8FvP"
   },
   "outputs": [],
   "source": [
    "#!pip install  gensim==4.0.1 # if you decide to use the gensim library and the sample codes below, you would need gensim version >=4.0.1 to be installed \n",
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oywLsqBYrayZ"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import logging\n",
    "from re import sub\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pa0dlD102lbp"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Initialize logging.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwQBtaxk5rXS",
    "outputId": "b8f4ad5a-7bc6-4106-9d09-fa454ebf177a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Import and download stopwords from NLTK.\n",
    "nltk.download('stopwords')  # Download stopwords list.\n",
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mlGyZeZ2Nle"
   },
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    # you may decide to add additional steps here \n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vP1s_4hMpqAn",
    "outputId": "3160a3a1-f1de-4d39-81b8-a19ab0b83312"
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with open('data/sample_repository.json') as in_file:\n",
    "    test_data = json.load(in_file)\n",
    "\n",
    "titles = [item[0] for item in test_data['data']]\n",
    "documents = [item[1] for item in test_data['data']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aaVXOXGDpqD5"
   },
   "outputs": [],
   "source": [
    "query_s = 'Your queries here'\n",
    " \n",
    "\n",
    "\n",
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR_F5zkipqH7",
    "outputId": "4f47c364-0463-40df-83a3-c92bb10605db"
   },
   "outputs": [],
   "source": [
    "# Download and load the GloVe word vector embeddings\n",
    "\n",
    "if 'glove' not in locals():  # only load if not already in memory\n",
    "    glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtWEI622pqLa",
    "outputId": "9c5efad2-443b-4d50-bbd2-087c819288e5"
   },
   "outputs": [],
   "source": [
    "# Build the term dictionary, TF-idf model\n",
    "# Keep in mind that the search query must be in the dictionary as well, in case the terms do not overlap with the documents  \n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix. \n",
    "# The nonzero_limit enforces sparsity by limiting the number of non-zero terms in each column. \n",
    "# For my application, I got best results by removing the default value of 100\n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)  # , nonzero_limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fn9tPObFpqPJ",
    "outputId": "c47eef5c-dae4-4abf-cb00-f16facad770d"
   },
   "outputs": [],
   "source": [
    "# Compute similarity measure between the query and the documents.\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-XmAUHwpqSD"
   },
   "outputs": [],
   "source": [
    "# Output the similarity scores for top 5/10 documents and interpreat the findings and compare the results \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8at07gGAI-MH"
   },
   "source": [
    "# 3. BERT\n",
    "Use a bert model to create sentence embeddings and calculate the similarity between queries and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBSozs_tYFjg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raaQrNXUKL2o"
   },
   "outputs": [],
   "source": [
    "# compare the findings  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLH7jBj4KL7E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpX8H6_TEPOM"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SCS_3546_Assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
