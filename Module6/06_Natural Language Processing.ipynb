{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwsPppAAl6h3"
   },
   "source": [
    "# SCS 3546 Week 6 - Natural Language Processing (NLP)\n",
    "(NLP Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZA7DZwpl6h9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n33R4EgHl6h-"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nn4qFhfl6h-"
   },
   "source": [
    "- Develop some familiarity with key concepts in NLP\n",
    "- Understanding Regular Expressions, n-grams, annotators, word and document representation, language and topic modeling\n",
    "- Have a look at some of the features of the Natural Language Toolkit (NLTK), spaCy, gensim, coreNLP and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65ENNGOpl6h_"
   },
   "source": [
    "## What is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6egIKpAcl6h_"
   },
   "source": [
    "- Natural Language Processing (NLP) is the study of computational treatment of natural (human) language\n",
    "\n",
    "- NLP applications include:\n",
    "  - Search: Web, documents, autocomplete\n",
    "  - Editing: Spelling, grammar, style\n",
    "  - Dialog: Chatbots, assistants\n",
    "  - Writing: Index, concordance, table of contents\n",
    "  - Email: Spam filter, classification, prioritization\n",
    "  - Text mining: Summarization, knowledge extraction, medical diagnosis\n",
    "  - Law: Legal inference, precedent search, subpoena classification\n",
    "  - News: Event detection, fact checking, headline composition\n",
    "  - Attribution: Plagiarism detection, literary forensics, style coaching\n",
    "  - Sentiment analysis: Community morale monitoring, product review triage, customer care\n",
    "  - Behavior prediction: Finance, election forecasting, marketing\n",
    "  - Creative writing: Movie scripts, poetry, song lyrics\n",
    "  \n",
    "Source: Natural Language Processing in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36gh0VYll6iA"
   },
   "source": [
    "## Ambiguity in Natural Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvGB4n2El6iA"
   },
   "source": [
    "Ambiguity in language is one of the main issues in interpreting the concepts they represent. Consider the following sentences:\n",
    "\n",
    "- \"Teachers Strikes Idle Kids.\"\n",
    "- “Eats shoots and leaves”\n",
    "- \"Stolen Painting Found by Tree.\"\n",
    "- \"Local High School Dropouts Cut in Half\"\n",
    "\n",
    "\n",
    "Grammar is a description, not a prescription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdfLYcLll6iB"
   },
   "source": [
    "## Other Issues\n",
    "- Synonyms\n",
    "       A synonym is a word or phrase that means exactly or nearly the same as another lexeme in the same language.\n",
    "- Homonyms\n",
    "        In linguistics, homonyms, broadly defined, are words which sound alike or are spelled alike, but have different meanings.\n",
    "- Misspellings\n",
    "        an incorrect spelling\n",
    "- Sarcasm\n",
    "        the use of irony to mock or convey contempt.\n",
    "- Allegory\n",
    "        a story, poem, or picture that can be interpreted to reveal a hidden meaning, typically a moral or political one.\n",
    "- Dialects\n",
    "        a particular form of a language which is peculiar to a specific region or social group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFSr4lDxl6iC"
   },
   "source": [
    "## Corpus\n",
    "\n",
    "In linguistics, a corpus (plural corpora) or text corpus is a large and structured set of texts (nowadays usually electronically stored and processed).\n",
    "\n",
    "<table border=\"1\" class=\"docutils\" id=\"tab-corpora\">\n",
    "<colgroup>\n",
    "<col width=\"31%\">\n",
    "<col width=\"18%\">\n",
    "<col width=\"51%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr><th class=\"head\">Corpus</th>\n",
    "<th class=\"head\">Compiler</th>\n",
    "<th class=\"head\">Contents</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr><td>Brown Corpus</td>\n",
    "<td>Francis, Kucera</td>\n",
    "<td>15 genres, 1.15M words, tagged, categorized</td>\n",
    "</tr>\n",
    "<tr><td>CESS Treebanks</td>\n",
    "<td>CLiC-UB</td>\n",
    "<td>1M words, tagged and parsed (Catalan, Spanish)</td>\n",
    "</tr>\n",
    "<tr><td>Chat-80 Data Files</td>\n",
    "<td>Pereira &amp; Warren</td>\n",
    "<td>World Geographic Database</td>\n",
    "</tr>\n",
    "<tr><td>CMU Pronouncing Dictionary</td>\n",
    "<td>CMU</td>\n",
    "<td>127k entries</td>\n",
    "</tr>\n",
    "<tr><td>CoNLL 2000 Chunking Data</td>\n",
    "<td>CoNLL</td>\n",
    "<td>270k words, tagged and chunked</td>\n",
    "</tr>\n",
    "<tr><td>CoNLL 2002 Named Entity</td>\n",
    "<td>CoNLL</td>\n",
    "<td>700k words, pos- and named-entity-tagged (Dutch, Spanish)</td>\n",
    "</tr>\n",
    "<tr><td>CoNLL 2007 Dependency Treebanks (sel)</td>\n",
    "<td>CoNLL</td>\n",
    "<td>150k words, dependency parsed (Basque, Catalan)</td>\n",
    "</tr>\n",
    "<tr><td>Dependency Treebank</td>\n",
    "<td>Narad</td>\n",
    "<td>Dependency parsed version of Penn Treebank sample</td>\n",
    "</tr>\n",
    "<tr><td>Floresta Treebank</td>\n",
    "<td>Diana Santos et al</td>\n",
    "<td>9k sentences, tagged and parsed (Portuguese)</td>\n",
    "</tr>\n",
    "<tr><td>Gazetteer Lists</td>\n",
    "<td>Various</td>\n",
    "<td>Lists of cities and countries</td>\n",
    "</tr>\n",
    "<tr><td>Genesis Corpus</td>\n",
    "<td>Misc web sources</td>\n",
    "<td>6 texts, 200k words, 6 languages</td>\n",
    "</tr>\n",
    "<tr><td>Gutenberg (selections)</td>\n",
    "<td>Hart, Newby, et al</td>\n",
    "<td>18 texts, 2M words</td>\n",
    "</tr>\n",
    "<tr><td>Inaugural Address Corpus</td>\n",
    "<td>CSpan</td>\n",
    "<td>US Presidential Inaugural Addresses (1789-present)</td>\n",
    "</tr>\n",
    "<tr><td>Indian POS-Tagged Corpus</td>\n",
    "<td>Kumaran et al</td>\n",
    "<td>60k words, tagged (Bangla, Hindi, Marathi, Telugu)</td>\n",
    "</tr>\n",
    "<tr><td>MacMorpho Corpus</td>\n",
    "<td>NILC, USP, Brazil</td>\n",
    "<td>1M words, tagged (Brazilian Portuguese)</td>\n",
    "</tr>\n",
    "<tr><td>Movie Reviews</td>\n",
    "<td>Pang, Lee</td>\n",
    "<td>2k movie reviews with sentiment polarity classification</td>\n",
    "</tr>\n",
    "<tr><td>Names Corpus</td>\n",
    "<td>Kantrowitz, Ross</td>\n",
    "<td>8k male and female names</td>\n",
    "</tr>\n",
    "<tr><td>NIST 1999 Info Extr (selections)</td>\n",
    "<td>Garofolo</td>\n",
    "<td>63k words, newswire and named-entity SGML markup</td>\n",
    "</tr>\n",
    "<tr><td>NPS Chat Corpus</td>\n",
    "<td>Forsyth, Martell</td>\n",
    "<td>10k IM chat posts, POS-tagged and dialogue-act tagged</td>\n",
    "</tr>\n",
    "<tr><td>PP Attachment Corpus</td>\n",
    "<td>Ratnaparkhi</td>\n",
    "<td>28k prepositional phrases, tagged as noun or verb modifiers</td>\n",
    "</tr>\n",
    "<tr><td>Proposition Bank</td>\n",
    "<td>Palmer</td>\n",
    "<td>113k propositions, 3300 verb frames</td>\n",
    "</tr>\n",
    "<tr><td>Question Classification</td>\n",
    "<td>Li, Roth</td>\n",
    "<td>6k questions, categorized</td>\n",
    "</tr>\n",
    "<tr><td>Reuters Corpus</td>\n",
    "<td>Reuters</td>\n",
    "<td>1.3M words, 10k news documents, categorized</td>\n",
    "</tr>\n",
    "<tr><td>Roget's Thesaurus</td>\n",
    "<td>Project Gutenberg</td>\n",
    "<td>200k words, formatted text</td>\n",
    "</tr>\n",
    "<tr><td>RTE Textual Entailment</td>\n",
    "<td>Dagan et al</td>\n",
    "<td>8k sentence pairs, categorized</td>\n",
    "</tr>\n",
    "<tr><td>SEMCOR</td>\n",
    "<td>Rus, Mihalcea</td>\n",
    "<td>880k words, part-of-speech and sense tagged</td>\n",
    "</tr>\n",
    "<tr><td>Senseval 2 Corpus</td>\n",
    "<td>Pedersen</td>\n",
    "<td>600k words, part-of-speech and sense tagged</td>\n",
    "</tr>\n",
    "<tr><td>Shakespeare texts (selections)</td>\n",
    "<td>Bosak</td>\n",
    "<td>8 books in XML format</td>\n",
    "</tr>\n",
    "<tr><td>State of the Union Corpus</td>\n",
    "<td>CSPAN</td>\n",
    "<td>485k words, formatted text</td>\n",
    "</tr>\n",
    "<tr><td>Stopwords Corpus</td>\n",
    "<td>Porter et al</td>\n",
    "<td>2,400 stopwords for 11 languages</td>\n",
    "</tr>\n",
    "<tr><td>Swadesh Corpus</td>\n",
    "<td>Wiktionary</td>\n",
    "<td>comparative wordlists in 24 languages</td>\n",
    "</tr>\n",
    "<tr><td>Switchboard Corpus (selections)</td>\n",
    "<td>LDC</td>\n",
    "<td>36 phonecalls, transcribed, parsed</td>\n",
    "</tr>\n",
    "<tr><td>Univ Decl of Human Rights</td>\n",
    "<td>United Nations</td>\n",
    "<td>480k words, 300+ languages</td>\n",
    "</tr>\n",
    "<tr><td>Penn Treebank (selections)</td>\n",
    "<td>LDC</td>\n",
    "<td>40k words, tagged and parsed</td>\n",
    "</tr>\n",
    "<tr><td>TIMIT Corpus (selections)</td>\n",
    "<td>NIST/LDC</td>\n",
    "<td>audio files and transcripts for 16 speakers</td>\n",
    "</tr>\n",
    "<tr><td>VerbNet 2.1</td>\n",
    "<td>Palmer et al</td>\n",
    "<td>5k verbs, hierarchically organized, linked to WordNet</td>\n",
    "</tr>\n",
    "<tr><td>Wordlist Corpus</td>\n",
    "<td>OpenOffice.org et al</td>\n",
    "<td>960k words and 20k affixes for 8 languages</td>\n",
    "</tr>\n",
    "<tr><td>WordNet 3.0 (English)</td>\n",
    "<td>Miller, Fellbaum</td>\n",
    "<td>145k synonym sets</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "[Source: Natural Language Processing with Python,Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTM27zvKl6iF"
   },
   "source": [
    "### Stop Words\n",
    "\n",
    "Stop words are generally the most common words in a language; there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1627232309117,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "batdqTpzl6iG",
    "outputId": "e159426c-7ac8-461d-e762-e1bc6ffd720f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: ignore\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.warn(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8HxI6Drl6iH"
   },
   "source": [
    "### WordNet\n",
    "WordNet is **a large lexical database of English**. Nouns, verbs, adjectives and adverbs are grouped into **sets of cognitive synonyms (synsets)**, each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. \n",
    "\n",
    "<img src=\"https://www.nltk.org/images/wordnet-hierarchy.png\">\n",
    "\n",
    "[Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch02.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3625,
     "status": "ok",
     "timestamp": 1627232312727,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "lbu7MdDkl6iH",
    "outputId": "8b4becfd-4222-4abc-b0ec-ce41b449ca85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "Synset('ambulance.n.01')\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "motorcar = wordnet.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "print(types_of_motorcar[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1627232312728,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "ps1ilO-Fl6iI",
    "outputId": "47aae3a8-2e8e-479d-ef31-e521e561eb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap', 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wagon']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgcTHt4cl6iI"
   },
   "source": [
    "### Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1627232312728,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "YOe3uMrgl6iI",
    "outputId": "3353300a-710d-40e2-e945-350ebc353b77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1627232312783,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "KuxB2AcYl6iJ",
    "outputId": "181abfa1-4116-4678-f13f-2b2bf0360e71"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'[Poems by William Blake 1789]\\n\\n \\nSONGS OF INNOCENCE AND OF EXPERIENCE\\nand THE BOOK of THEL\\n\\n\\n SONGS '"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.raw('blake-poems.txt')[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Swd2Hs_1l6iJ"
   },
   "source": [
    "### Web Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1627232312784,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "fGbNRNVll6iJ",
    "outputId": "531536cb-1ecb-4ac9-fd7f-7c6a27b14a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['firefox.txt',\n",
       " 'grail.txt',\n",
       " 'overheard.txt',\n",
       " 'pirates.txt',\n",
       " 'singles.txt',\n",
       " 'wine.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "nltk.download('webtext')\n",
    "webtext.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1627232436678,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "I1LmdQmYl6iK",
    "outputId": "10650b02-06d3-4fcb-fa80-9d0a88f87fc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cookie', 'Manager', ':', '\"', 'Don', \"'\", 't', 'allow', 'sites', 'that', 'set', 'removed', 'cookies', 'to', 'set', 'future', 'cookies', '\"', 'should', 'stay', 'checked', 'When', 'in', 'full', 'screen', 'mode', 'Pressing', 'Ctrl', '-', 'N', 'should', 'open', 'a', 'new', 'browser', 'when', 'only', 'download', 'dialog', 'is', 'left', 'open', 'add', 'icons', 'to', 'context', 'menu', 'So', 'called', '\"', 'tab', 'bar', '\"', 'should', 'be', 'made', 'a', 'proper', 'toolbar', 'or', 'given', 'the', 'ability', 'collapse', '/', 'expand', '.'], ['[', 'XUL', ']', 'Implement', 'Cocoa', '-', 'style', 'toolbar', 'customization', '.'], ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webtext.sents('firefox.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxMivvtKl6iK"
   },
   "source": [
    "### Brown Text\n",
    "\n",
    "15 genres, 1.15M words, tagged, categorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1627232313564,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "A95hJ8JHl6iK",
    "outputId": "f12c4a24-8b78-4c3b-e582-11dd86ca8f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1627232313565,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "sJeR5taol6iL",
    "outputId": "b5da0366-1d39-4d2f-bd81-296cd5335a98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4gVD2mRl6iL"
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1627232316652,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "hSKnIarQl6iM",
    "outputId": "6d7df814-ea68-48c6-ac61-9bb424234ef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgGcj9xxl6iN"
   },
   "source": [
    "### Pronouncing Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1627232316812,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "xbzQpwNpl6iN",
    "outputId": "f60f9299-3884-43d1-f707-b8fdee4dfb77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('abreu', ['AH0', 'B', 'R', 'UW1']),\n",
       " ('abridge', ['AH0', 'B', 'R', 'IH1', 'JH']),\n",
       " ('abridged', ['AH0', 'B', 'R', 'IH1', 'JH', 'D']),\n",
       " ('abridgement', ['AH0', 'B', 'R', 'IH1', 'JH', 'M', 'AH0', 'N', 'T']),\n",
       " ('abridges', ['AH0', 'B', 'R', 'IH1', 'JH', 'AH0', 'Z']),\n",
       " ('abridging', ['AH0', 'B', 'R', 'IH1', 'JH', 'IH0', 'NG']),\n",
       " ('abril', ['AH0', 'B', 'R', 'IH1', 'L']),\n",
       " ('abroad', ['AH0', 'B', 'R', 'AO1', 'D']),\n",
       " ('abrogate', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T']),\n",
       " ('abrogated', ['AE1', 'B', 'R', 'AH0', 'G', 'EY2', 'T', 'IH0', 'D'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')\n",
    "entries = nltk.corpus.cmudict.entries()\n",
    "entries[300:310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMOeAZYal6iN"
   },
   "source": [
    "### Comparative Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1627232316973,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "dAHuNIvWl6iO",
    "outputId": "d3a9c098-fb56-4b29-cf00-1557a34c42bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/swadesh.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr']"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import swadesh\n",
    "nltk.download('swadesh')\n",
    "swadesh.fileids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1627232316974,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "EJNqJ6qHl6iO",
    "outputId": "d24d54a9-a558-4f37-b772-e2b82896f9a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(207, ('nous', 'we'))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr2en = swadesh.entries(['fr', 'en'])\n",
    "len(fr2en) , fr2en[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69x69ogdl6iO"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzdrmuGDl6iO"
   },
   "source": [
    "### Concept\n",
    "- Split on white space (in most languages)\n",
    "- **Regular expressions** and **finite state automata** are often used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlTxB96yl6iP"
   },
   "source": [
    "### Issues in Tokenization\n",
    "\n",
    "Consider: “There’s a moon in the sky.  It’s called The Moon.\" (B52's)\n",
    "\n",
    "- Special cases: Names (particularly multi-word), initials, hyphenated words, abbreviations, special forms (dates, phone numbers, URLs, etc.)\n",
    "- Punctuation\n",
    "- Contractions: is \"isn’t\" one word or two (many tokenizers treat as two)\n",
    "- Named Entities\n",
    "- Rare words\n",
    "- Stop words\n",
    "\n",
    "- Other languages\n",
    "  - German: Compounds words such as schadenfreude\n",
    "  - Chinese: average of about 2 symbol/word; greedy readings work quite well but there are better algos\n",
    "  - Japanese: kanji, hirigana, katakana, romanji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuWKezghl6iP"
   },
   "source": [
    "### Popular Python tokenizers:\n",
    "  - NLTK\n",
    "  - SpaCy\n",
    "  - Keras\n",
    "  - CoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH3_NYPWl6iP"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 710,
     "status": "ok",
     "timestamp": 1627232317673,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "5lo5zFVgl6iP",
    "outputId": "e50b4020-0e1a-45cf-a869-2e223fdbfd73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "['DENNIS: Listen, strange women lying in ponds distributing swords\\nis no basis for a system of government.', 'Supreme executive power derives from\\na mandate from the masses, not from some farcical aquatic ceremony.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = sent_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1627232317674,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "Dot_CyBAl6iP",
    "outputId": "9eee5a72-5ffd-424a-ddf3-1499904e935e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'lying', 'in', 'ponds', 'distributing', 'swords', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'masses', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ4lyLf3l6iQ"
   },
   "source": [
    "## Normalization\n",
    "\n",
    "The concept is to replace similar words with a single token and reduce word vector size. This task is a kind of dimensionality reduction. It reduces the processing cost and the likelihood of overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVK5N7gWl6iQ"
   },
   "source": [
    "### Normalization Techniques\n",
    "- Case folding: Force all lower case \n",
    "  - Can make named entity resolution more difficult\n",
    "  - Becoming less common as a result\n",
    "- Stemming: Crude chopping of affixes\n",
    "  - e.g. remove -s or -ing at end\n",
    "  - Can cut vocabulary size in half (or more if aggressive)\n",
    "  - Many algoritms: Porter’s is most common English stemmer and has a lot of knowledge of English hardcoded in it\n",
    "  - Useful for search where we are looking for similar, not exact matches (will improve recall but reduce precision)\n",
    "- Lemmatization: Extraction of the base form\n",
    "  - e.g. “are”, “am”, “is” replaced with “be”\n",
    "  - Better for most applications than stemmers which might take “better” and convert it to “bet”\n",
    "- Hashtag expansions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAUhpADyl6iQ"
   },
   "source": [
    "### When to Use Stemming vs. Lemmatization\n",
    "- If you want the recall benefit of stemming try putting a lemmatizer before the stemmer\n",
    "- NLTK lemmatizer uses the Princeton WordNet graph of word meanings\n",
    "- Newer packages like SpaCy don’t provide a stemmer, only. a lemmatizer\n",
    "- Stemmers and lemmatizers (like stop words) are being less used all the time as computers become more powerful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pasbh5oLl6iQ"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1627232317674,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "MRma93Jgl6iR",
    "outputId": "2f091dfc-1c4b-43d7-f5e9-e9c075ead5c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1627232317676,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "7EwLigKhl6iR",
    "outputId": "097b6299-41b9-4796-b084-53108dc2d6d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1627232317822,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "Ih1vwpWWoisd",
    "outputId": "797d1851-aa1e-4b9b-88b1-29c99a44cea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1627232318014,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "vZTnC8tBl6iS",
    "outputId": "06eaf4d9-ccfe-4974-bc6d-e30b959900da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DENNIS', 'DENNIS'),\n",
       " ('Listen', 'Listen'),\n",
       " ('strange', 'strange'),\n",
       " ('women', 'woman'),\n",
       " ('lying', 'lie'),\n",
       " ('ponds', 'pond'),\n",
       " ('distributing', 'distribute'),\n",
       " ('swords', 'sword'),\n",
       " ('is', 'be'),\n",
       " ('basis', 'basis'),\n",
       " ('system', 'system'),\n",
       " ('government', 'government'),\n",
       " ('Supreme', 'Supreme'),\n",
       " ('executive', 'executive'),\n",
       " ('power', 'power'),\n",
       " ('derives', 'derive'),\n",
       " ('mandate', 'mandate'),\n",
       " ('masses', 'mass'),\n",
       " ('not', 'not'),\n",
       " ('farcical', 'farcical'),\n",
       " ('aquatic', 'aquatic'),\n",
       " ('ceremony', 'ceremony')]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Getting the part of speech of a word\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "        print(tag)\n",
    "\n",
    "tagged_sent = nltk.pos_tag(tokens) # Gets part of speech\n",
    "lmtzr = WordNetLemmatizer()\n",
    "[(x[0],lmtzr.lemmatize(x[0], get_wordnet_pos(x[1])))\n",
    " for x in tagged_sent if get_wordnet_pos(x[1]) is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pZKnKVDl6iT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i4ie582l6iT"
   },
   "source": [
    "## Annotators \n",
    "\n",
    "<img src=\"https://stanfordnlp.github.io/CoreNLP/assets/images/pipeline.png\">\n",
    "\n",
    "[source CoreNLP Website](https://stanfordnlp.github.io/CoreNLP/)\n",
    "\n",
    "\n",
    "you can try different annotators [here](http://corenlp.run/)\n",
    "\n",
    "For spaCy you should run following commands: \n",
    "\n",
    "* `pip install spacy`\n",
    "* `python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hX6L7KLl6iT"
   },
   "source": [
    "### POS (Part Of Speech) Tagger \n",
    "\n",
    "Part-of-speech tagging (POS tagging or PoS tagging or POST), also called **grammatical tagging or word-category disambiguation**, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.[source](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=19I5kJPRfQ6OdnWVHr9BqYWYB11B9Mrcl\">\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1627232318017,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "gvqK3uzZl6iT",
    "outputId": "013eaccf-069b-4264-9344-cc3630d73a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1627232318165,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "5PgySR4yl6iU",
    "outputId": "400d91d4-8a55-4a40-c99e-a7396b316740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJsqxtoPl6iU"
   },
   "source": [
    "### Name Entity Recognition \n",
    "\n",
    "**Named-entity recognition (NER)** is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text. [source](https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "\n",
    "In information extraction, a **named entity** is a **real-world object**, such as persons, locations, organizations, products, etc., that can be denoted with a proper name. It can be abstract or have a physical existence. Examples of named entities include Barack Obama, New York City, Volkswagen Golf, or anything else that can be named. [source](https://en.wikipedia.org/wiki/Named_entity)\n",
    "\n",
    "<img src=\"https://stanfordnlp.github.io/CoreNLP/assets/images/ner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFoFepJql6iU"
   },
   "source": [
    "### Constituent Parsing\n",
    "\n",
    "Constituent parsing is the task of **recognizing a sentence and assigning a syntactic structure** to it.[source](https://stanfordnlp.github.io/CoreNLP/parse.html) \n",
    "\n",
    "\n",
    "[Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch07.html)\n",
    "\n",
    "One of the common problems is **Ubiquitous Ambiguity**. Please consider example below\n",
    "\n",
    "\n",
    "\n",
    "[Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch07.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_lk5Iupl6iV"
   },
   "source": [
    "### Dependency Parsing \n",
    "\n",
    "Dependency parsing is the task of extracting a dependency parse of a sentence that represents its grammatical structure and **defines the relationships between “head” words and words**, which modify those heads.[source](http://nlpprogress.com/english/dependency_parsing.html)\n",
    "\n",
    "<img src=\"https://stanfordnlp.github.io/CoreNLP/assets/images/depparse.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Lu2wve_l6iV"
   },
   "source": [
    "### Coreference Resolution\n",
    "\n",
    "Coreference resolution is the task of finding **all expressions that refer to the same entity** in a text. \n",
    "\n",
    "<img src=\"https://nlp.stanford.edu/projects/corefexample.png\">\n",
    "\n",
    "[source:nlpCore website](https://nlp.stanford.edu/projects/coref.shtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-Zj9hQgl6iV"
   },
   "source": [
    "### Open information extraction (open IE) \n",
    "\n",
    "**Open information extraction (open IE)** refers to the **extraction of relation tuples**, typically binary relations, from plain text, such as (Mark Zuckerberg; founded; Facebook).[source](https://nlp.stanford.edu/software/openie.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qq1IJJEfl6iV"
   },
   "source": [
    "##  Regular Expressions\n",
    "\n",
    "A **regular expression** or **regex** is a **sequence of characters that define a search pattern**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRDnpCHpl6iW"
   },
   "source": [
    "- Helps you find and match the patterns in text\n",
    "    - Finding all the email addresses in webpage\n",
    "    - Email addresses are strings that has exactly one @ sign, and at least one . in the part after the @\n",
    "    - Based on the above description (which is too general but mostly enough for basic check) the regular expression for an email address is as follows\n",
    "    - Email pattern: **[^@]+@[^@]+\\.[^@]+**\n",
    "    - The above pattern means some at least one character which is not @ ([^@]+), followed by an @ sign, followed by at least one character which is not @ ([^@]+), followed by a single . , and again followed by at least one character which is not @ ([^@]+)\n",
    "    - A more comprehensive pattern that does not allow spaces inside email adresses is as follows:\n",
    "    - Email pattern: **[^@|\\s]+@[^@]+\\.[^@|\\s]+**\n",
    "    \n",
    "- Some Regular expression syntax examples \n",
    "    - **.**\tMatches any character\n",
    "    - **^abc**\tMatches some pattern abc at start of a string\n",
    "    - **abc$**\tMatches some pattern abc at the end of a string\n",
    "    - **[abc]**\tMatches one of a set of characters\n",
    "    - **[A-Z0-9]**\tMatches one of a range of character\n",
    "    - **ed|ing|s**\tMatches one of the specified strings\n",
    "    - **\\***\tMatches zero or more of the previous item\n",
    "    - **\\+**\tMatches one or more of the previous item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAkWb2ILl6iW"
   },
   "source": [
    "###  Regular Expressions in Python\n",
    "\n",
    "- Take a crash course to learn the most commonly used syntax for writing the patterns\n",
    "    - [Regex tutorial — A quick cheatsheet by examples](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)\n",
    "    - [Python Regex Cheatsheet](https://www.debuggex.com/cheatsheet/regex/python)\n",
    "    - In Python **re** library can be used. It has many methods and the following methos are more common\n",
    "        - re.match()\n",
    "        - re.search()\n",
    "    - The match() function only checks if the RE matches at the beginning of the string while search() will scan forward through the string for a match. It’s important to keep this distinction in mind. Remember, match() will only report a successful match which will start at 0; if the match wouldn’t start at zero, match() will not report it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1627232318166,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "pFITDmTYl6iW",
    "outputId": "d1daa8e8-88b8-4a25-d29b-434986ebb6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5)\n",
      "None\n",
      "(0, 5)\n",
      "(2, 7)\n"
     ]
    }
   ],
   "source": [
    "# Let's see how we can make use of re library to extract part of text\n",
    "import re\n",
    "print(re.match('super', 'superstition').span())\n",
    "print(re.match('super', 'insuperable'))\n",
    "print(re.search('super', 'superstition').span())\n",
    "print(re.search('super', 'insuperable').span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfEtcgusl6iW"
   },
   "source": [
    "### Finding email addresses in a text document using Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ic68EUl6iX"
   },
   "source": [
    "The following code shows how we can use **re** library to extract all the **valid** email addresses from a source docuements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1627232801060,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "xG3emde3l6iX",
    "outputId": "1bd49490-2825-4f94-cc68-e7a99e45ca0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example@me.com', 'me@example.com']"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding email addresses in a text file\n",
    "import re\n",
    "string=\"Hello friend, You can send me an email either to example@me.com or to me@example.com or me[at]gmail[dot]com\"\n",
    "# findall returns all non-overlapping matches of pattern in string, as a list of strings. \n",
    "res = re.findall(\"[^@|\\s]+@[^@]+\\.[^@|\\s]+\",string) \n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQj69uK9l6iX"
   },
   "source": [
    "### Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1627232318491,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "hdxMYIJnl6iY",
    "outputId": "009995d3-6546-4dbc-fca7-6a9df6e9a045",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/ieer.zip.\n",
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('ieer')\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4nq_b8xl6iY"
   },
   "source": [
    "## Language Models\n",
    "\n",
    "A statistical language model is a **probability distribution over sequences of words**. Given such a sequence, say of length m, it assigns a probability ${\\displaystyle P(w_{1},\\ldots ,w_{m})}$ to the whole sequence.\n",
    "\n",
    "The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases \"recognize speech\" and \"wreck a nice beach\" sound similar, but mean different things.\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Language_model)\n",
    "- What is the probability of the next word given what we’ve seen so far?\n",
    "- Can’t just count instances: too many sentences; never have enough data\n",
    "- So we use a simplifying assumption: The Markov property (only the previous few words matter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxh-r-39l6iY"
   },
   "source": [
    "## n-Grams\n",
    "\n",
    "In the fields of computational linguistics and probability, an n-gram is a **continuous sequence of n items from a given sample of text or speech**.[source](https://en.wikipedia.org/wiki/N-gram)\n",
    "\n",
    "- The Distributional Hypothesis (1950’s): There is a link between how words are distributed and what they mean\n",
    "- Sapir-Whorf Hypothesis: The structure of a language determines a native speaker's perception and categorization of experience\n",
    "- Collocations: Words that go together to express a concept\n",
    "- Concordances: An alpabetical index that shows words in their context\n",
    "- n-grams are sequences of words found in natural language text\n",
    "- Useful for translation, spelling correction, speech recognition, question answering\n",
    "- Consider the meaning of “not old” vs. not and old individually: perhaps we should add “not old” to our vocabulary if it occurs\n",
    "- We can add 2, 3, etc.-grams\n",
    "- Can have letter n-grams as well (e.g. used internally in DBMS’s for wildcard lookups) but for our purposes we are only concerned with word n-grams\n",
    "- No point in trying to generate all possible n-grams in advance (combinatorial explosion) so we are only interested in the ones that actually occur in our corpus\n",
    "- Consider very rare collocations: Can’t determine their frequency of occurrence so not useful for classification problems\n",
    "- Consider very common collocations (e.g. “is a”): Carry almost no information (but useful for language detection)\n",
    "- A vocabulary of about 20,000 words is sufficient to track 95% of words in a corpus of tweets, blog posts and news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1627232318492,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "coBkH2-9l6iY",
    "outputId": "8b767c8b-9498-4470-e378-da6493232348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DENNIS:', 'Listen,')\n",
      "('Listen,', 'strange')\n",
      "('strange', 'women')\n",
      "('women', 'lying')\n",
      "('lying', 'in')\n",
      "('in', 'ponds')\n",
      "('ponds', 'distributing')\n",
      "('distributing', 'swords')\n",
      "('swords', 'is')\n",
      "('is', 'no')\n",
      "('no', 'basis')\n",
      "('basis', 'for')\n",
      "('for', 'a')\n",
      "('a', 'system')\n",
      "('system', 'of')\n",
      "('of', 'government')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government'\n",
    "n = 2\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "\n",
    "for grams in sixgrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-cbhcThl6iY"
   },
   "source": [
    "## Representing Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upIRT3u5l6iZ"
   },
   "source": [
    "- Primary \n",
    "    - One Hot Encoding \n",
    "    \n",
    "- Advanced Word Embeddings\n",
    "    - WordToVec\n",
    "    - Glove\n",
    "    - TagLM\n",
    "    - ELMO\n",
    "    - GPT\n",
    "    - BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKMRUkENl6iZ"
   },
   "source": [
    "#### One Hot Encoding  Example\n",
    "\n",
    "Following you can find an example for hot-encoding with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1502,
     "status": "ok",
     "timestamp": 1627232319990,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "LWWDHUP-l6iZ",
    "outputId": "bb11de1d-b634-4cca-dd3a-1d63f122ed4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60,\n",
       " 311,\n",
       " 302,\n",
       " 268,\n",
       " 303,\n",
       " 277,\n",
       " 352,\n",
       " 57,\n",
       " 250,\n",
       " 134,\n",
       " 381,\n",
       " 277,\n",
       " 124,\n",
       " 233,\n",
       " 217,\n",
       " 60,\n",
       " 79]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sentence = 'DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government'\n",
    "\n",
    "tf.keras.preprocessing.text.one_hot(\n",
    "    sentence,\n",
    "    400,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' '\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoPrBPHMl6ia"
   },
   "source": [
    "## Representing Documents\n",
    "### Pre-Deep Learning Approaches\n",
    "- Bag of Words\n",
    "- Bags of n-Grams\n",
    "- TF-IDF vectors\n",
    "- topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biW-i6-Zl6ia"
   },
   "source": [
    "### Bag of Words or n-Grams\n",
    "\n",
    "The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words,\n",
    "\n",
    "- We would like to model words, n-grams and documents in a way that is amenable to computer processing\n",
    "- First attempt\n",
    "  - One Hot encoding for words and n-grams\n",
    "  - Bag of Words (BOW) encoding for documents: sum or logical-OR of word vectors\n",
    "- Advantages:\n",
    "  - Numeric representation\n",
    "  - Simple to compute\n",
    "  - Easy to interpret and use\n",
    "  - Can compare two documents by comparing their BOW encodings\n",
    "- Disadvantages:\n",
    "  - Loses context and therefore meaning\n",
    "  - Hugely wasteful of memory space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40708,
     "status": "ok",
     "timestamp": 1627232360684,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "O17VyG-gl6ia",
    "outputId": "ebd6232d-01fc-4aab-b171-233a917e8dbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "dct = Dictionary(dataset)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1627232360690,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "LthzA-DDl6ib",
    "outputId": "49459eaf-cb0a-4e79-91b6-a8671407fc5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 184\n",
      "abacus 1\n",
      "abilities 1\n",
      "ability 3\n",
      "able 7\n",
      "abnormal 1\n",
      "abolished 1\n",
      "abolition 1\n",
      "about 12\n",
      "above 2\n"
     ]
    }
   ],
   "source": [
    "for word_id,count in corpus[0][:10]:\n",
    "    print(dct[word_id], count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQghbqq_l6ib"
   },
   "source": [
    "### TF-IDF\n",
    "In information retrieval, tf–idf or TFIDF, short for **term frequency–inverse document frequency**, is a **numerical statistic** that is intended to **reflect how important a word is to a document in a collection or corpus**.\n",
    "- Let's say we want to divide up a corpus (collection) of documents into similar clusters\n",
    "- How should we decide how similar two documents are?\n",
    "  - How many words they have in common\n",
    "  - How specialized those words are\n",
    "- To capture these two aspects we need two measures for each word in our vocabulary:\n",
    "  - Term Frequency: How frequently the word occurs in each document\n",
    "  - Document Frequency: How often the word occurs in our corpus\n",
    "- We can combine these measures as Term Frequency / Document Frequency (called TF-IDF for Term Frequency, Inverse Document Frequency)\n",
    "- the Document Frequency is usually measured on a log scale\n",
    "\n",
    "\n",
    "$$\n",
    "{\\text { tfidf }(t, d, D)=t f(t, d) \\cdot \\text { idf }(t, D)}$$\n",
    "\n",
    "$$\\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRaCpxygl6ib"
   },
   "outputs": [],
   "source": [
    "model = TfidfModel(corpus)  # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huM8MFcyl6ib"
   },
   "outputs": [],
   "source": [
    "vector = model[corpus[0]]  # apply model to the first corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1627232362844,
     "user": {
      "displayName": "simon tavasoli",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhfuwjBt5kVYXtsFK9tOFStg9HG4jRLyzishGuf3Q=s64",
      "userId": "06730416496601132709"
     },
     "user_tz": 240
    },
    "id": "sPFcIHTal6ib",
    "outputId": "99bec060-3333-4618-9ae8-1e584e101138"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abacus 0.006704047545684609\n",
      "abilities 0.0030255603220721273\n",
      "ability 0.003156168449586299\n",
      "able 0.0036673470201144674\n",
      "abnormal 0.004575122435127926\n",
      "abolished 0.0028052608258295926\n",
      "abolition 0.004064820137019515\n",
      "about 0.00014963587508918375\n",
      "above 0.0007492665180478759\n",
      "absence 0.004142807322609117\n"
     ]
    }
   ],
   "source": [
    "for word_id,weight in vector[:10]:\n",
    "    print(dct[word_id], weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faYZyxcvl6ie"
   },
   "source": [
    "## Similarity Measure \n",
    "\n",
    "### Cosine Similarity\n",
    "- We can measure the similarity of vectors (such as TF-IDF vectors) using Cosine Similarity\n",
    "- The more similar the vectors are, the smaller the angle there should be between them\n",
    "- The cosine similarity of two vectors, x and y, is easily calculated using dot product \n",
    "\n",
    "\\begin{align}\n",
    "cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}\n",
    "\\end{align}\n",
    "- Cosine similarity is a number that runs between 0 (nothing in common) to 1 (identical) for TF-IDF vectors (Note: Here identical means identical TF-IDF, not necessarily identical documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8x_KAU1l6ie"
   },
   "source": [
    "## NLP Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mnUrGr2l6ie"
   },
   "source": [
    "Natural Language processing is usually organized into a pipeline of operations that parse and analyze the text.\n",
    "The figure below shows an example of a NLP pipeline:\n",
    "<img src=\"https://miro.medium.com/max/1838/1*CbzCcP3XFtYVJmWowZLugQ.png\">\n",
    "[source: medium.com](https://medium.com/mlearning-ai/basic-steps-in-natural-language-processing-pipeline-763cd299dd99)\n",
    "\n",
    "Additional simple examples:\n",
    "\n",
    "- [NLP Pipeline: Building an NLP Pipeline, Step-by-Step](https://suneelpatel-in.medium.com/nlp-pipeline-building-an-nlp-pipeline-step-by-step-7f0576e11d08)\n",
    "\n",
    "- [NLP Text Preprocessing and Cleaning Pipeline in Python](https://towardsdatascience.com/nlp-text-preprocessing-and-cleaning-pipeline-in-python-3bafaf54ac35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFZzHiFmvQmQ"
   },
   "outputs": [],
   "source": [
    "'''Normalization '''\n",
    "text = '''The Blog Authorship Corpus consists of the collected posts of 19,320 bloggers gathered from blogger.com in August 2004. \n",
    "The corpus incorporates a total of 681,288 posts and over 140 million words - or approximately 35 posts and 7250 words per person [1]. \n",
    "Each blog is presented as a separate file, the name of which indicates a blogger id# and the blogger�s self-provided gender, age, industry and astrological sign. (All are labeled for gender and age but for many, industry and/or sign is marked as unknown.)'''\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFv_ZLjbvUYZ"
   },
   "outputs": [],
   "source": [
    "''' And remove punctuation '''\n",
    "import re\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text) \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsJOpkoxvUlu"
   },
   "outputs": [],
   "source": [
    "'''Tokenize words'''\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVd1MALhvUsv"
   },
   "outputs": [],
   "source": [
    "'''Tokenize sentences'''\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9u7wvXg6vUzJ"
   },
   "outputs": [],
   "source": [
    "'''Stop word removal '''\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Normalize text\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "# Tokenize text\n",
    "words = word_tokenize(text)\n",
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5tlMPjdvU9H"
   },
   "outputs": [],
   "source": [
    "'''POS and NER '''\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenize text\n",
    "sentence = word_tokenize(text)\n",
    "# tag each word with part of speech\n",
    "pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVAsU7KdvVFE"
   },
   "outputs": [],
   "source": [
    "'''use ne_chunk to find named entities '''\n",
    "# tokenize, pos tag, then recognize named entities in text\n",
    "tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(tree[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItO5roaovryF"
   },
   "outputs": [],
   "source": [
    "'''Stemming and Lemmatization '''\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "words = word_tokenize(text)\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Reduce words to their stems\n",
    "lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in lemmed]\n",
    "\n",
    "\n",
    "print(\"Stemmed:\", stemmed)\n",
    "print(\"Lemmas\", lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk8xjc--vw9i"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"Natural Language Processing or NLP is a field of Artificial Intelligence.\",\n",
    "        \"NLP gives the machines the ability to read, understand and derive meaning from human languages\",\n",
    "        \"Data Scientists work with tons of data, and many times that data includes natural language text.\",\n",
    "        \"Modern organizations work with huge amounts of data.\",\n",
    "        \"Is AI a bad thing ?\"]\n",
    "# initialize count vectorizer object\n",
    "# use your own tokenize function\n",
    "\n",
    "results = []\n",
    "for sentence in corpus:\n",
    "    sentence_results = []\n",
    "    for s in sentence:\n",
    "        sentence_results.append(nltk.word_tokenize(sentence))\n",
    "    results.append(sentence_results)\n",
    "vect = CountVectorizer(tokenizer=results)\n",
    "\n",
    "\n",
    "# get counts of each token (word) in text data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()\n",
    "# view token vocabulary and counts\n",
    "document_term_matrix = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHoxU6Uhv17t"
   },
   "outputs": [],
   "source": [
    "''' TF-IDF'''\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# initialize tf-idf transformer object\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# use counts from count vectorizer results to compute tf-idf values\n",
    "tfidf = transformer.fit_transform(X)\n",
    "# convert sparse matrix to numpy array to view\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gG7lsDcv4yi"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# initialize tf-idf vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# compute bag of word counts and tf-idf values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# convert sparse matrix to numpy array to view\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU3_bZBTl6ie"
   },
   "source": [
    "# The Natural Language Toolkit: NLTK\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n",
    "\n",
    "- Originally developed in 2001 at the University of Pennsylvania\n",
    "- Version 3.3 released in 2018\n",
    "- Natural Language Processing with Python book: https://www.nltk.org/book/  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84LnRLUml6ie"
   },
   "source": [
    "## References\n",
    "- http://nlpprogress.com/english/dependency_parsing.html\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n",
    "- https://wordnet.princeton.edu/\n",
    "- https://www.nltk.org/\n",
    "- https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition/tree/master/notebooks\n",
    "- https://en.wikipedia.org/wiki/Regular_expression\n",
    "- Lane, Howard & Hapke. Natural Language Processing in Action. Manning. 2019.\n",
    "- Jurafsky & Martin. Speech and Language Processing, 3rd Ed. https://web.stanford.edu/~jurafsky/slp3/\n",
    "- SpaCy: https://spacy.io/\n",
    "- gensim: https://radimrehurek.com/gensim/\n",
    "- [Source Natural Language Processing with Python Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/ch07.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXliWZFzl6if"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jzdrmuGDl6iO",
    "mlTxB96yl6iP",
    "TuWKezghl6iP",
    "pVK5N7gWl6iQ",
    "kAUhpADyl6iQ",
    "hJsqxtoPl6iU",
    "TFoFepJql6iU",
    "n_lk5Iupl6iV",
    "_Lu2wve_l6iV",
    "S-Zj9hQgl6iV",
    "UAkWb2ILl6iW",
    "AfEtcgusl6iW",
    "fQj69uK9l6iX",
    "lKMRUkENl6iZ",
    "biW-i6-Zl6ia",
    "uQghbqq_l6ib"
   ],
   "name": "06 - Natural Language Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
