{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkbtx-Xf3lEN"
   },
   "source": [
    "# SCS 3546 Week 7 - Deep Models for Text\n",
    "(NLP Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C-OhxHUjhZk"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbGxGBn93lEP"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s28S3AVh3lEP"
   },
   "source": [
    "- Develop familiarity with several of the key concepts in Deep Learning models for NLP\n",
    "- Understanding different word embeddings and their differences \n",
    "- Understand how to leverage word embedding in a classification task (Sentiment Analysis: Positive/Negative) \n",
    "- Build knowledge of how seq2seq models can be used for NLP tasks such as translation\n",
    "- Become familiar with Attention, Self-Attention and Transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPLlBgdr3lEP"
   },
   "source": [
    "## Applications of Deep Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D2iphxBjskc"
   },
   "source": [
    "- Text Classification and Categorization\n",
    "- Named Entity Recognition\n",
    "- Part of Speech Tagging\n",
    "- Semantic Parsing and Query Answering\n",
    "- Summarization\n",
    "- Paraphrasing Detection\n",
    "- Synonym generation for search\n",
    "- Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvnvSq6R3lEP"
   },
   "source": [
    "# Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmT7EiKM3lEP"
   },
   "source": [
    "## Encoding with Integers\n",
    "\n",
    "As we discussed last week, the traditional method of encoding words was to start by assigning an integer index to each unique word in the vocabulary then create vectors of a length equal to the size of the vocabulary:\n",
    "\n",
    "- word encoding: all entries zero except the one at the index of the word, which was set to 1\n",
    "- document encoding: each entry one (or a count of the number of occurrences) if the word is present in the document or zero if not\n",
    "\n",
    "This has several advantages:\n",
    "\n",
    "- Every word is represented by an integer that can be used as a fast look up index to additional information about the word (such as its text string)\n",
    "- Every word vector is a fixed-sized data structure which makes them easier to process\n",
    "- Comparing two words for equality is very fast\n",
    "- We can compare the similarity of documents using techniques such as TF-IDF and cosine similarity\n",
    "\n",
    "There are several things about this that are less than ideal:\n",
    "\n",
    "- The vectors don't capture any word meaning\n",
    "- The order of the words is arbitrary\n",
    "- The vectors are large and therefore wasteful of computer memory and CPU\n",
    "- Vectors with continuous (floating point) values are more amenable to use with neural nets than those with discrete values\n",
    "\n",
    "We would prefer an approach where:\n",
    "\n",
    "- Words that are close together in their encodings are similar in some sense\n",
    "- The vectors are short and dense rather than long and sparse\n",
    "- The vectors capture some aspect of the meaning of the words\n",
    "- The different uses of a particular token (e.g. \"bow\" as in \"bow and arrow\" and \"bow of a ship\") can be disambiguated\n",
    "- The vectors are more amenable for use with neural nets\n",
    "\n",
    "If we could find such an encoding, then we could use the vectors for:\n",
    "\n",
    "- Classifying the document as being about a topic without having to compare it to others already associated with a topic\n",
    "- Measure the sentiment of a document without having to hand-engineer sentiment ratings for each word in the vocabulary\n",
    "- Use the meaning of words and sentences to provide better automated translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeB8hGvT3lEQ"
   },
   "source": [
    "## Word Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8U9z5x0j7Z2"
   },
   "source": [
    "\n",
    "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "\n",
    "Following the list of popular embeddings\n",
    "\n",
    "- **GloVe**\n",
    "- **Word2Vec**\n",
    "- **FastText**\n",
    "- **TagLM**\n",
    "- **Bert**\n",
    "- **Albert**\n",
    "- **RoBert**\n",
    "- **GPT**\n",
    "- **XLNet**\n",
    "- **Performer**\n",
    "\n",
    "And this list will be continued since every day a new embedding will be introduced :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qepeusHm3lER"
   },
   "source": [
    "### Word2vec \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzwH-WQIFTR6"
   },
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mH6UuDzRcpmW"
   },
   "source": [
    "Word2Vec is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks. there are two main methods for learning the embeddings: **Bag-of-Words** and **Skip-gram**\n",
    "\n",
    "* **Continuous Bag-of-Words Model** predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n",
    "\n",
    "* **Continuous Skip-gram Model** predicts words within a certain range before and after the current word in the same sentence. A worked example of this is given below. \n",
    "\n",
    "In this Module we will discuss how Skip-gram works; Consider the sentence \"The quick brown fox jumps over the lazy dog\". The context words for each of the 8 words of this sentence are defined by a window size. The window size determines the span of words on either side of a **target_word** that can be considered **context word**. We wish to find the probability distribution of each of the vocabulary words of appearing in a window of a fixed size on either side of a given word. Following image demonstrates how we can build a training sample from source text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv2P4Wxl3lER"
   },
   "source": [
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=12StHx-FVVPiuJnRgNul-ObwKoSJdpGmS\" ></center>\n",
    "\n",
    "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word. For a sequence of words w1, w2, ... wT, the objective can be written as the average log probability\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p\\left(w_{t+j} \\mid w_{t}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1n6P1HxxfS5hGt-lG9IZWd_oyYAhkgKJK\" ></center>\n",
    "\n",
    "\n",
    "If O is the event of a word occuring anywhere in the window and C is the event of having a particular centre word, we wish to find $$P(O|C) = \\exp(u_O^Tv_C)/\\sum_{w=1}^V\\exp(u_w^Tv_C)$$ where u and v are the vector representations and V is the size of the vocabulary.\n",
    "\n",
    "Computing the denominator of this formulation involves performing a full softmax over the entire vocabulary words which is often is hard to compute. There are ways to make the training more efficient; \n",
    "\n",
    "The [Noise Contrastive Estimation loss](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf) function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modelling the word distribution, NCE loss can be simplified to use negative sampling.\n",
    "\n",
    "The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution Pn(w) of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.\n",
    "\n",
    "A negative sample is defined as a (target_word, context_word) pair such that the context_word does not appear in the window_size neighborhood of the target_word. \n",
    "\n",
    "After the training process, we will use these weights as a vector representation for each word. These embeddings will be useful for downstream tasks like classification or clustering. Further, you can reduce the dimension of these embeddings and plot them in 3D. [here](http://projector.tensorflow.org/) you can find the example of it.\n",
    "\n",
    "The interesting properties of Word2Vec model is that you can do is doing linear algebra arithmetic with words. For example, a popular example described in lectures and introduction papers is:\n",
    "\n",
    "`queen = (king - man) + woman`\n",
    "\n",
    "You can also take a look at the result by changing man to boy and woman to girl and we also have:\n",
    "\n",
    "`queen = (king - boy) + girl`\n",
    "\n",
    "In addition to the names, you can apply it to the verbs:\n",
    "\n",
    "`walking = (walked - swam) + swimming`\n",
    "\n",
    "Figure below shows this property by these examples:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ONDACyoRRbqbgeHo1loisHXC-Lf-p9xJ\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSEv9ZIBEqI0"
   },
   "source": [
    "#### Generating Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Niy_h2vnqX-B"
   },
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lK0e4YUVqdEp"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wmdO_MEIpaM"
   },
   "source": [
    "##### Compile all steps into one function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLKwNAczHsKg"
   },
   "source": [
    "###### Skip-gram Sampling table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUUK3uDtFNFE"
   },
   "source": [
    "A large dataset means larger vocabulary with higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as `the`, `is`, `on`) don't add much useful information  for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPtbv7zNP7Dx"
   },
   "source": [
    "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to  generate a word-frequency rank based probabilistic sampling table and pass it to `skipgrams` function. Take a look at the sampling probabilities for a `vocab_size` of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rn9zAnDccyRg",
    "outputId": "779d60b8-5b30-46b0-c00b-dcffa40d73ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
      " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
     ]
    }
   ],
   "source": [
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHvSptcPk5fp"
   },
   "source": [
    "`sampling_table[i]` denotes the probability of sampling the i-th most common word in a dataset. The function assumes a [Zipf's distribution](https://en.wikipedia.org/wiki/Zipf%27s_law) of the word frequencies for sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRHMssMmHgH-"
   },
   "source": [
    "Key point: The `tf.random.log_uniform_candidate_sampler` already assumes that the vocabulary frequency follows a log-uniform (Zipf's) distribution. Using these distribution weighted sampling also helps approximate the Noise Contrastive Estimation (NCE) loss with simpler loss functions for training a negative sampling objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj--8RFK6fgW"
   },
   "source": [
    "###### Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy5hl4lQ0B2M"
   },
   "source": [
    "Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63INISDEX1Hu"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shvPC8Ji2cMK"
   },
   "source": [
    "##### Prepare training data for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5mbZsZu6uKg"
   },
   "source": [
    "With an understanding of how to work with one sentence for a skip-gram negative sampling based Word2Vec model, you can proceed to generate training examples from a larger list of sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFlikI6L26nh"
   },
   "source": [
    "##### Download text corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEFavOgN98al"
   },
   "source": [
    "You will use a text file of Shakespeare's writing for this tutorial. Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFkitxzVVaAi"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOsbLq8a37dr"
   },
   "source": [
    "Read text from the file and take a look at the first few lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfgnsUw3ofMD",
    "outputId": "29071eb4-2d48-47ad-d74d-fdbff6c77dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n"
     ]
    }
   ],
   "source": [
    "with open(path_to_file) as f: \n",
    "  lines = f.read().splitlines()\n",
    "for line in lines[:20]:\n",
    "  print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTNZYqUs5C2V"
   },
   "source": [
    "Use the non empty lines to construct a `tf.data.TextLineDataset` object for next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViDrwy-HjAs9"
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfsc88zE9upk"
   },
   "source": [
    "##### Vectorize sentences from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfgZo8zR94KK"
   },
   "source": [
    "You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": [
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g92LuvnyBmz1"
   },
   "source": [
    "Call `adapt` on the text dataset to create vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seZau_iYMPFT"
   },
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg2z7eeHMnH-"
   },
   "source": [
    "Once the state of the layer has been adapted to represent the text corpus, the vocabulary can be accessed with `get_vocabulary()`. This function returns a list of all vocabulary tokens sorted (descending) by their frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgw9pTA7MRaU",
    "outputId": "e1fd2f58-7864-4900-bdd9-2eed8d6a3b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOQ30Tx6KA2G"
   },
   "source": [
    "The vectorize_layer can now be used to generate vectors for each element in the `text_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUVYrDp0araQ"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YyH_SYzB72p"
   },
   "source": [
    "##### Obtain sequences from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFUQLX0_KaRC"
   },
   "source": [
    "You now have a `tf.data.Dataset` of integer encoded sentences. To prepare the dataset for training a Word2Vec model, flatten the dataset into a list of sentence vector sequences. This step is required as you would iterate over each sentence in the dataset to produce positive and negative examples. \n",
    "\n",
    "Note: Since the `generate_training_data()` defined earlier uses non-TF python/numpy functions, you could also use a `tf.py_function` or `tf.numpy_function` with `tf.data.Dataset.map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGXoOh9y11pM",
    "outputId": "efcb591d-8aa5-4144-bfc8-b085381bd5bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDc4riukLTqg"
   },
   "source": [
    "Take a look at few examples from `sequences`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZf1RIbB2Dfb",
    "outputId": "9dad044b-0d50-43f0-f3f7-2fa85f07722e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
      "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
      "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
      "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
      "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzSOjNwCWNh"
   },
   "source": [
    "##### Generate training examples from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BehvYr-nEKyY"
   },
   "source": [
    "`sequences` is now a list of int encoded sentences. Just call the `generate_training_data()` function defined earlier to generate training examples for the Word2Vec model. To recap, the function iterates over each word from each sequence to collect positive and negative context words. Length of target, contexts and labels should be same, representing the total number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44DJ22M6nX5o",
    "outputId": "a0aa84c2-bee0-47d9-d41c-19eed6e0b9fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32777/32777 [00:09<00:00, 3534.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64612 64612 64612\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "print(len(targets), len(contexts), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97PqsusOFEpc"
   },
   "source": [
    "##### Configure the dataset for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnFVySViQTj"
   },
   "source": [
    "To perform efficient batching for the potentially large number of training examples, use the `tf.data.Dataset` API. After this step, you would have a `tf.data.Dataset` object of `(target_word, context_word), (label)` elements to train your Word2Vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbu8PxPSnVY2",
    "outputId": "390b8108-e06d-416e-fc21-30342bb8b36a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyrNX6Fs6K3F"
   },
   "source": [
    "Add `cache()` and `prefetch()` to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5Ueg6bcFPVL",
    "outputId": "31f57dd2-e658-43ac-a81d-7831cdb3941a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S-CmUMszyEf"
   },
   "source": [
    "##### Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQFqaBMPwBqC"
   },
   "source": [
    "The Word2Vec model can be implemented as a classifier to distinguish between true context words from skip-grams and false context words obtained through negative sampling. You can perform a dot product between the embeddings of target and context words to obtain predictions for labels and compute loss against true labels in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oc7kTbiwD9sy"
   },
   "source": [
    "###### Subclassed Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jvr9pM1G1sQN"
   },
   "source": [
    "Use the [Keras Subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) to define your Word2Vec model with the following layers:\n",
    "\n",
    "\n",
    "* `target_embedding`: A `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a target word. The number of parameters in this layer are `(vocab_size * embedding_dim)`.\n",
    "* `context_embedding`: Another `tf.keras.layers.Embedding` layer which looks up the embedding of a word when it appears as a context word. The number of parameters in this layer are the same as those in `target_embedding`, i.e. `(vocab_size * embedding_dim)`.\n",
    "* `dots`: A `tf.keras.layers.Dot` layer that computes the dot product of target and context embeddings from a training pair.\n",
    "* `flatten`: A `tf.keras.layers.Flatten` layer to flatten the results of `dots` layer into logits.\n",
    "\n",
    "With the subclassed model, you can define the `call()` function that accepts `(target, context)` pairs which can then be passed into their corresponding embedding layer. Reshape the `context_embedding` to perform a dot product with `target_embedding` and return the flattened result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAwuIqqw7-7"
   },
   "source": [
    "Key point: The `target_embedding` and `context_embedding` layers can be shared as well. You could also use a concatenation of both embeddings as the final Word2Vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9ec-sS6xd8Z"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "    self.dots = Dot(axes=(3, 2))\n",
    "    self.flatten = Flatten()\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    word_emb = self.target_embedding(target)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    dots = self.dots([context_emb, word_emb])\n",
    "    return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RLKz9LFECXu"
   },
   "source": [
    "###### Define loss function and compile model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3Md-9QanqBM"
   },
   "source": [
    "For simplicity, you can use `tf.keras.losses.CategoricalCrossEntropy` as an alternative to the negative sampling loss. If you would like to write your own custom loss function, you can also do so as follows:\n",
    "\n",
    "``` python\n",
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
    "```\n",
    "\n",
    "It's time to build your model! Instantiate your Word2Vec class with an embedding dimension of 128 (you could experiment with different values). Compile the model with the `tf.keras.optimizers.Adam` optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekQg_KbWnnmQ"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "num_ns=10\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3MUMrluqNX2"
   },
   "source": [
    "Also define a callback to log training statistics for tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9d-ftBCeEZIR"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wEBotlGZ7B"
   },
   "source": [
    "Train the model with `dataset` prepared above for some number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmC1BJalEZIY",
    "outputId": "e3c8b7e0-4956-4728-a8c1-d3f8a6d31d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 2s 20ms/step - loss: 1.6083 - accuracy: 0.2326\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5891 - accuracy: 0.5554\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.5415 - accuracy: 0.6123\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.4578 - accuracy: 0.5836\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 1.3586 - accuracy: 0.5861\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.2618 - accuracy: 0.6087\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.1723 - accuracy: 0.6405\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 1.0895 - accuracy: 0.6744\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 1.0126 - accuracy: 0.7069\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.9411 - accuracy: 0.7364\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8746 - accuracy: 0.7616\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.8130 - accuracy: 0.7845\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7561 - accuracy: 0.8041\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.7038 - accuracy: 0.8218\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.6558 - accuracy: 0.8370\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.6119 - accuracy: 0.8507\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.5719 - accuracy: 0.8634\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.5353 - accuracy: 0.8746\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 1s 15ms/step - loss: 0.5021 - accuracy: 0.8853\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 1s 16ms/step - loss: 0.4718 - accuracy: 0.8945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5b83f89a90>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wze38jG57XvZ"
   },
   "source": [
    "Tensorboard now shows the Word2Vec model's accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "22E9eqS55rgz",
    "outputId": "ea092784-729d-4f8a-c5b4-afe326212645"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
       "            url.searchParams.set('tensorboardColab', 'true');\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awF3iRQCZOLj"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/word2vec_tensorboard.png\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaDW2tIIz8fL"
   },
   "source": [
    "##### Embedding lookup and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWzdmUzS8Sl4"
   },
   "source": [
    "Create and save the vectors and metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1T8KcThhIU8-"
   },
   "source": [
    "Download the `vectors.tsv` and `metadata.tsv` to analyze the obtained embeddings in the [Embedding Projector](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lUsjQOKMIV2z",
    "outputId": "95307a0a-5e24-441b-8b6a-363d1e120b33"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3760546a-9ac3-4adc-949c-dd6a9a4b6ff5\", \"vectors.tsv\", 6113139)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_9e15bb3c-f348-4c0a-9926-3ba3152bebd2\", \"metadata.tsv\", 28737)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TZ66lhf3lET"
   },
   "source": [
    "#### Sentiment analysis of Movie Reviews Using Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne7mlYXjStV_"
   },
   "source": [
    "Following the sample implementation, however you can find the alternative [here](https://www.tensorflow.org/tutorials/text/text_classification_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NbfXJkEjzdan",
    "outputId": "954dcde5-b3b5-4a46-dce0-28502034674e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras \n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhXuKWba1pTK",
    "outputId": "f5a82473-19bd-4b6d-dca9-a0c64990bf02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 50)                48832000  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                816       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 48,832,833\n",
      "Trainable params: 833\n",
      "Non-trainable params: 48,832,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-es-dim50-with-normalization/1\", output_shape=[50],\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370,
     "referenced_widgets": [
      "30146e76e00d4276828897f27f556ce8",
      "3da731aa490146f489fdea7d2435d789",
      "6242ad4637aa44f0bd80f754784e07eb",
      "7cf3d189d10544ca90954358f315b6b6",
      "f3c9697bef2a464f8f955fafcfff7838",
      "074b8cce066a49dd8b5c1e023de21dff",
      "8c80fbcb2c5a49e2b8b73d730fcad011",
      "5a2a0e12c3a6401f9f20cffb3170f376",
      "837c1387e7cd4090931e8588018a636f",
      "5150d5e1538647dc9a673d567fef2a72",
      "a0a21d201250419db610bf33db0a1c19",
      "12d1925600a0404ba43358b8ebda2822",
      "0a29d599f6b94469a2d65f606f80fa0d",
      "540308e19aee43b58bab940326e65a36",
      "61506a8a75d94c8283e6d84451208ddb",
      "0ec297f8167f4be7b999b8ef41c5162a",
      "8d99d12840b04adf81e370ae15d853a4",
      "334c6cdf0c5644e1974ad8f15674175b",
      "d9649a0a1c5b4ad8b8a6cd465730cff0",
      "8304d7c355334d8f99fe7eec48292ba5",
      "d317ae17625c484c878284c466ff59e1",
      "8e3bcacfe0b2455c9ddf33104313732c",
      "665392b42eab42b8a7b2934916179157",
      "133615349f36491195a3ba73aa5753a4",
      "139cd977ff434d428cd5524e7a97f52e",
      "ab1283224d2c4f619f4c1a63f85313ae",
      "1d52cca5686f46d7b5111b5ceab2c921",
      "4ea5040984ce46609940b229d0ac8f84",
      "0486eae28e0e49fb9562fb88565939ba",
      "bc8d749de2dd4f079762ce03a43acaae",
      "cc09ed776ad842c7990a27ee36aaa1fe",
      "42df465e1d7f4026af66e244f3ae410b",
      "938790171f544b099e0ad712a0e4d0d1",
      "46fc795543054d9f8d9e58ded25cc371",
      "0ff6e7261fe848b182b9beab7cd28a96",
      "1613e50847a24c649d5154cdb4f80946",
      "c436a10513b74acd9cb7c1cc5aacc041",
      "30b0320d755d42e2b7c51640af912617",
      "c965b54a4c8d4ed0bf3a5ef8c0e18b04",
      "0fa76a3de5a04d8182b34d247b7b7b70",
      "9801932150934306bb27e6a89204c246",
      "5aeb95f3c078404f9d2bc34c1aa9524d",
      "871887f1ef834d3dbe8da6a2ad14e009",
      "52f64d67cc2e41ffaae55268cc544c0b",
      "af8130d4b659468b8997cd84ded28f25",
      "375d7fdc75a84c29b990c1bc3ffd81a4",
      "3c3bd1723cac4a14ae3bd9f49857e9e8",
      "c5d8796060074b3698f235a9d5e7208c",
      "d936e45a4b1842529e1b9c26284630b1",
      "c1af9a3c56274c56aa65fe0860a09bf9",
      "afc6a92d763942488ed2c96d4668052c",
      "b554d6348d484d098345a7bb6f812cc8",
      "0191caae8a16458e9042c869550e3db1",
      "0898b64130d24b53ad4b464b967e42b7",
      "29c4fcf911a544ecb5d989da4d23ea37",
      "aedbe3594e65427ead4b6b2cce898da8",
      "68f4ca31235c48ce851c6bade3a40f02",
      "063055bb350444b1a6aadcc2a5611ec0",
      "9a7d0754efee4988aaa1e1225c0eea18",
      "22474f3161f044efa5ab0543c3530c48",
      "b6836df9d1c24f9aaea79cbf214f34d5",
      "2adc2fbe57aa443f90a707efc60a34d8",
      "ebb6bb23b4534ffcaa34d3d90451a042",
      "f82dedde52ca4f538db8db64c5787baf"
     ]
    },
    "id": "PRbHE7BF_Pqw",
    "outputId": "5c271a71-8539-48de-d1cf-8e10d7ed2093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30146e76e00d4276828897f27f556ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837c1387e7cd4090931e8588018a636f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d99d12840b04adf81e370ae15d853a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteRANV04/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139cd977ff434d428cd5524e7a97f52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938790171f544b099e0ad712a0e4d0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteRANV04/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9801932150934306bb27e6a89204c246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d936e45a4b1842529e1b9c26284630b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteRANV04/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f4ca31235c48ce851c6bade3a40f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\r"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSTB1UBhBBn5",
    "outputId": "a7615c86-b870-4e50-f44c-e118a4cfa760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "  for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "      print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "      print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTdEqVGNHeS2"
   },
   "outputs": [],
   "source": [
    "train_datasets = datasets[\"train\"].batch(128)\n",
    "test_datasets = datasets[\"test\"].batch(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cCNb8mFcBJPN",
    "outputId": "066265e2-cc70-4611-da75-fe8a623206fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 7s 32ms/step - loss: 0.6379 - accuracy: 0.6351 - val_loss: 0.6114 - val_accuracy: 0.6674\n",
      "Epoch 2/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.6022 - accuracy: 0.6788 - val_loss: 0.5943 - val_accuracy: 0.6857\n",
      "Epoch 3/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5932 - accuracy: 0.6880 - val_loss: 0.5888 - val_accuracy: 0.6906\n",
      "Epoch 4/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5886 - accuracy: 0.6922 - val_loss: 0.5858 - val_accuracy: 0.6921\n",
      "Epoch 5/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5859 - accuracy: 0.6949 - val_loss: 0.5841 - val_accuracy: 0.6944\n",
      "Epoch 6/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5837 - accuracy: 0.6961 - val_loss: 0.5824 - val_accuracy: 0.6956\n",
      "Epoch 7/20\n",
      "196/196 [==============================] - 6s 31ms/step - loss: 0.5819 - accuracy: 0.6968 - val_loss: 0.5811 - val_accuracy: 0.6968\n",
      "Epoch 8/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5798 - accuracy: 0.6995 - val_loss: 0.5795 - val_accuracy: 0.6978\n",
      "Epoch 9/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5788 - accuracy: 0.6998 - val_loss: 0.5788 - val_accuracy: 0.6992\n",
      "Epoch 10/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5779 - accuracy: 0.7006 - val_loss: 0.5787 - val_accuracy: 0.6996\n",
      "Epoch 11/20\n",
      "196/196 [==============================] - 6s 33ms/step - loss: 0.5767 - accuracy: 0.7011 - val_loss: 0.5780 - val_accuracy: 0.7011\n",
      "Epoch 12/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5742 - accuracy: 0.7012 - val_loss: 0.5760 - val_accuracy: 0.7024\n",
      "Epoch 13/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5718 - accuracy: 0.7046 - val_loss: 0.5762 - val_accuracy: 0.7020\n",
      "Epoch 14/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5699 - accuracy: 0.7056 - val_loss: 0.5762 - val_accuracy: 0.7027\n",
      "Epoch 15/20\n",
      "196/196 [==============================] - 6s 31ms/step - loss: 0.5694 - accuracy: 0.7083 - val_loss: 0.5769 - val_accuracy: 0.6992\n",
      "Epoch 16/20\n",
      "196/196 [==============================] - 6s 31ms/step - loss: 0.5679 - accuracy: 0.7097 - val_loss: 0.5789 - val_accuracy: 0.6977\n",
      "Epoch 17/20\n",
      "196/196 [==============================] - 6s 31ms/step - loss: 0.5665 - accuracy: 0.7108 - val_loss: 0.5782 - val_accuracy: 0.6965\n",
      "Epoch 18/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5658 - accuracy: 0.7105 - val_loss: 0.5760 - val_accuracy: 0.6995\n",
      "Epoch 19/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5649 - accuracy: 0.7098 - val_loss: 0.5755 - val_accuracy: 0.7023\n",
      "Epoch 20/20\n",
      "196/196 [==============================] - 6s 32ms/step - loss: 0.5646 - accuracy: 0.7106 - val_loss: 0.5758 - val_accuracy: 0.6996\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-2)\n",
    "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_datasets, validation_data=test_datasets, epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbmsUMLxHHM6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIQbPBfjUsdZ"
   },
   "source": [
    "Try a different model with different types of LSTM, RNN networks and compare the performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rs4HWDlmUs_j"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1Kvx5DKUtC0"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzKm2JyBUtFo"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqdfsuL8UtH7"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xcB20CRWuJW"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71-ZsjtkULdQ"
   },
   "source": [
    "Following you have the access to Word2Vec model. Try to run sentiment prediction exercise with the Word2Vec embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAboLZsMUTWY",
    "outputId": "87206eef-a017-4f5a-8ec6-3e3e3c804ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_1 (KerasLayer)   (None, 500)               504687500 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                8016      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 504,695,533\n",
      "Trainable params: 8,033\n",
      "Non-trainable params: 504,687,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/Wiki-words-500-with-normalization/2\",\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAtm3yEEUpZ0"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLGw3oLMWnQE"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huPFILubWnUM"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2QWpXnFWnch"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xH-aUk65WngJ"
   },
   "outputs": [],
   "source": [
    "### Your response here ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkhYbCWw3lEW"
   },
   "source": [
    "#### Word Vector Caveats\n",
    "\n",
    "- Words could be same set of characters but mean differently in different context\n",
    "- We don't have right representation for unknown words \n",
    "- Word vectors are biased by the biases in the text they are trained on\n",
    "- We know much more about the world than can be derived from how words are used in context: we have a lifetime of experience and are keenly aware that we live in a world of cause and effect\n",
    "- Words that are used in the same context are not necessarily interchangeable (e.g. one and five, fast and slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIicnD1P3lEW"
   },
   "source": [
    "### TagLM\n",
    "\n",
    "The TagLM paper by Matthew Peters, These are contextualized word embeddings learned from the internal states of a deep bidirectional language model. For example, the word “queen” will not have the same embedding in “Queen of the United Kingdom” and in “queen bee”.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1mpjabIOqjeC90DPO92qc4Muz1c8Tvcrd\">\n",
    "\n",
    "[source:TagLM paper](https://arxiv.org/pdf/1705.00108.pdf)\n",
    "\n",
    "\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJfh-ovf3lEW"
   },
   "source": [
    "Following you can see all the components in TagLM\n",
    "<center>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1o-_xWorpaV-b4FiFqJo5rfDsObm2HvKY\">\n",
    "\n",
    "\n",
    "[source:TagLM paper](https://arxiv.org/pdf/1705.00108.pdf)\n",
    "\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r7liViq3lEW"
   },
   "source": [
    "### ELMO\n",
    "\n",
    "- [Deep contextualized word representations. NAACL 2018](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "- Breakout version of word token vectors or contextual word vectors\n",
    "- Learn word token vectors using long contexts not context  windows (here, whole sentence, could be longer)\n",
    "- Learn a deep Bi-NLM and use all its layers in prediction\n",
    "\n",
    "- Train a bidirectional LM Aim at performant but not overly large LM:\n",
    "    - Use 2 biLSTM layers\n",
    "    - Use character CNN to build initial word representation (only)\n",
    "    - 2048 char n-gram filters and 2 highway layers, 512 dim projection\n",
    "    - User 4096 dim hidden/cell LSTM states with 512 dim  projections to next input\n",
    "    - Use a residual connection\n",
    "    - Tie parameters of token input and output (softmax) and tie  these \n",
    "    - between forward and backward LMs\n",
    "    \n",
    "\n",
    "$$\n",
    "\\begin{aligned} R_{k} &=\\left\\{\\mathbf{x}_{k}^{L M}, \\overrightarrow{\\mathbf{h}}_{k, j}^{L M}, \\overleftarrow{\\mathbf{h}}_{k, j}^{L M} | j=1, \\ldots, L\\right\\} \\\\ &=\\left\\{\\mathbf{h}_{k, j}^{L M} | j=0, \\ldots, L\\right\\} \\\\ \\mathbf{E L M o}_{k}^{t a s k} &=E\\left(R_{k} ; \\Theta^{t a s k}\\right)=\\gamma^{t a s k} \\sum_{j=0}^{L} s_{j}^{t a s k} \\mathbf{h}_{k, j}^{L M} \\end{aligned}\n",
    "$$\n",
    "\n",
    "- First run biLM to get representations for each word \n",
    "- Then let (whatever) end-task model use them\n",
    "    - Freeze weights of ELMo for purposes of supervised model\n",
    "    - Concatenate ELMo weights into task-specific model\n",
    "        - Details depend on task\n",
    "            - Concatenating into intermediate layer as for TagLM is typical\n",
    "            - Can provide ELMo representations again when producing outputs,  as in a question answering system\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_BRBP-g3lEW"
   },
   "source": [
    "# Sequence To Sequence Models\n",
    "\n",
    "- Sequence-to-sequence models convert one sequence to another\n",
    "- They read an entire sequence (e.g. a sentence), encode it into a more general representation, then decode it into a new sequence\n",
    "- The encoders and decoders are often RNN's\n",
    "- In a standard seq2seq model for Neural Machine Translation the input and output vectors are embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlHMCWa7Aeb_"
   },
   "source": [
    "### Machine Learning Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiT3pQim3lEX"
   },
   "source": [
    "- Machine Translation (MT) is the task of translating a sentence x  from one language (the source language) to a sentence y in  another language (the target language).\n",
    "- Alignment is one the issues in machine translation.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=14sAaaV4w6NH_teAPMqPC_EXOIRYkNSV6\" >\n",
    "\n",
    "[source: Lecture 8:\n",
    "Machine Translation,\n",
    "Sequence-to-sequence and Attention\n",
    "Abigail See](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)\n",
    "\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV0MvlK03lEX"
   },
   "source": [
    "You can think about Machine translation as building a conditional language model.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1HeD-exFROQRMsLHNR4oxZ9K9sqkFD3vT\" >\n",
    "\n",
    "\n",
    "[source:Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by Aurélien Chapter 16](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#)\n",
    "\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIfO5bJC3lEX"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=123Z7aJt0pk5CGlpUSFv4Wd7c4ukptoOQ\" >\n",
    "\n",
    "[source: Lecture 8:\n",
    "Machine Translation,\n",
    "Sequence-to-sequence and Attention\n",
    "Abigail See](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)\n",
    "\n",
    "\n",
    "</center>\n",
    "\n",
    "- RNN encoders take pairs of (input vector, previous hidden state) and the final hidden state becomes the representation to be decoded\n",
    "- An RNN decoder takes pairs (output vector, previous hidden state), the output vectors taken in order being the output sequence\n",
    "- The output at each step is a distribution across possible output words, so how do we choose?\n",
    "  - Greedy Search: Pick the one with the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlyHMPbn3lEX"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1mW_TFloszz8LCUE_Yp6JVizxcgRvs1P_\" >\n",
    "\n",
    "[source: Lecture 8:\n",
    "Machine Translation,\n",
    "Sequence-to-sequence and Attention\n",
    "Abigail See](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvOvXJP-3lEX"
   },
   "source": [
    "\n",
    "   - Ancestral Sampling: Sample from the distribution\n",
    "      - Beam Search: Find the k most likely (where $t$ is something like 5); generate $t$ next words for each; evaluate and keep $t$ best\n",
    "\n",
    "$$\n",
    "\\operatorname{score}\\left(y_{1}, \\ldots, y_{t}\\right)=\\sum_{i=1}^{t} \\log P_{\\mathrm{LM}}\\left(y_{i} | y_{1}, \\ldots, y_{i-1}, x\\right)\n",
    "$$\n",
    "      \n",
    "<center><img src=\"https://drive.google.com/uc?id=18c86k6lFz5f4_eqZigZ_mvzGhEy1ExB5\" >\n",
    "\n",
    "\n",
    "[source: Lecture 8:\n",
    "Machine Translation,\n",
    "Sequence-to-sequence and Attention\n",
    "Abigail See](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)  \n",
    "\n",
    "\n",
    "</center>\n",
    "\n",
    " - Google translates over 100B words/day\n",
    "- In 2014 NMT burst past other techniques\n",
    "- Advantages\n",
    "  - End-to-end training\n",
    "  - Distributed representations allow reuse of learned language features\n",
    "  - Ability to use much larger context than small-n-grams\n",
    "  - Better quality results\n",
    "- Google NMT 2016\n",
    "  - Far better\n",
    "  - Single system for all languages to languages\n",
    "  - Can do an ok job of translating A to C if it knows A to B and B to C without training A to C\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "360yqAj93lEX"
   },
   "source": [
    "### Issues with the basic seq2seq model\n",
    "- Embeddings like word2vec assume that collocation is important but don't make any finer distinction\n",
    "- Some words in the context are much more important than others\n",
    "- The words can be related in many ways: one describes another (adjectives and adverbs), one acts on another (subjects, objects, transitive verbs), one serves as a placeholder for another (pronouns)\n",
    "- Relationships can be far away; they don't need to be in the same sentence\n",
    "- Meaning unfurls itself slowly and retroactively; sometimes we can't understand what a sentence means until we get to the end of it; sometimes it only makes sense later or in the context of a lot that has come before\n",
    "- CNN's and RNN's suffer from information and processing bottlenecks\n",
    "- RNN's are:\n",
    "  - slow (unparallelizable)\n",
    "  - poor at using information that is not very close to the word being translated (the hidden state isn't big enough; LSTM's effectiveness drops rapidly for sentences beyond 30 words)\n",
    "  - hard to interpret the meaning of the state because it entangles many previous representations\n",
    "- Models without attention are usually good at producing grammatically correct output but occasionally throw in an unrelated word or antonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRr8GEnmAl0F"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmAQng2cC32m"
   },
   "source": [
    "First we download and unzip the dataset by using `pathlib` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQr8IdoNAtxN",
    "outputId": "68b05b2d-da55-4e6f-8e52-e96f229a4a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
    "\n",
    "lines = path_to_file.read_text(encoding='utf-8').splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1o4zpNJIC3K2",
    "outputId": "86002b2f-fc98-4f18-94f6-30e6a9c2d9ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVe.', 'Go.\\tVete.', 'Go.\\tVaya.', 'Go.\\tVáyase.']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nm8cfaWDmob"
   },
   "source": [
    "Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
    "* `encoder_input_data` is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "* `decoder_input_data` is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
    "* `decoder_target_data` is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BF3yb92kC3Ht"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABl89EyyC3FN",
    "outputId": "06e3c55b-fb75-444c-fc4d-b62f8b46ac91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 86\n",
      "Max sequence length for inputs: 17\n",
      "Max sequence length for outputs: 42\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yxFXtOWWC3Cm"
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQOsSqEdEazg"
   },
   "source": [
    "Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Slyj91e3C2_9"
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "meHQeAwZC29w"
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXwoXjzSC26o",
    "outputId": "f9576197-625a-4935-f9f9-4030874edb95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 38s 282ms/step - loss: 1.2808 - val_loss: 1.4297\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 33s 267ms/step - loss: 1.2221 - val_loss: 1.4119\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 1.2024 - val_loss: 1.3883\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 1.1785 - val_loss: 1.3574\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 1.1542 - val_loss: 1.3372\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 1.1314 - val_loss: 1.3140\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 1.0917 - val_loss: 1.2683\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 1.0502 - val_loss: 1.2251\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 1.0117 - val_loss: 1.1592\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.9753 - val_loss: 1.1515\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.9463 - val_loss: 1.1079\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.9180 - val_loss: 1.0763\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.8924 - val_loss: 1.0457\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.8703 - val_loss: 1.0306\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.8511 - val_loss: 1.0116\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.8347 - val_loss: 0.9761\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.8158 - val_loss: 0.9676\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 35s 276ms/step - loss: 0.8009 - val_loss: 0.9578\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.7851 - val_loss: 0.9456\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.7714 - val_loss: 0.9265\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.7592 - val_loss: 0.9180\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.7462 - val_loss: 0.9096\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.7335 - val_loss: 0.8920\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.7213 - val_loss: 0.8756\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.7081 - val_loss: 0.8652\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.6935 - val_loss: 0.8542\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.6813 - val_loss: 0.8538\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 34s 275ms/step - loss: 0.6703 - val_loss: 0.8387\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.6582 - val_loss: 0.8258\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.6485 - val_loss: 0.8149\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.6394 - val_loss: 0.8202\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.6299 - val_loss: 0.8078\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 36s 284ms/step - loss: 0.6223 - val_loss: 0.8031\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 35s 284ms/step - loss: 0.6157 - val_loss: 0.8043\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.6092 - val_loss: 0.7905\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 36s 284ms/step - loss: 0.6027 - val_loss: 0.7883\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 35s 280ms/step - loss: 0.5956 - val_loss: 0.7825\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 36s 287ms/step - loss: 0.5902 - val_loss: 0.7849\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 36s 285ms/step - loss: 0.5846 - val_loss: 0.7818\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.5804 - val_loss: 0.7767\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.5738 - val_loss: 0.7711\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.5686 - val_loss: 0.7695\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 35s 283ms/step - loss: 0.5639 - val_loss: 0.7708\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.5601 - val_loss: 0.7666\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.5546 - val_loss: 0.7648\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 0.5501 - val_loss: 0.7651\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 35s 278ms/step - loss: 0.5467 - val_loss: 0.7642\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 34s 276ms/step - loss: 0.5409 - val_loss: 0.7624\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 34s 272ms/step - loss: 0.5372 - val_loss: 0.7610\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.5324 - val_loss: 0.7583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff8bccf1210>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "g-WYsegAEkyg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ5uUhGKEnpQ"
   },
   "source": [
    "Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dELVol_REkvW"
   },
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hZumico7Eks2"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICMgur5IC23z",
    "outputId": "e8ad8938-32be-421f-86bc-0f0b949f1cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Ven.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Ven.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Ven.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Ven.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Homo.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: ¡Lo riento.\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Corre.\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: ¿Qué cante.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: ¡Despira.\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: ¡Despira.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFymXdwe3lEX"
   },
   "source": [
    "## Attention\n",
    "- The notion of *attention* is to have a model learn what parts of the context to pay the most attention to\n",
    "<center><img src=\"https://drive.google.com/uc?id=18fUzvF3Rh7G2IQGRqTwuEapfXDWXl7UP\" >\n",
    "\n",
    "[source:Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by Aurélien Chapter 16](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#)\n",
    "\n",
    "</center>\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\widetilde{\\mathbf{h}}_{(t)} &=\\sum_{i} \\alpha_{(i, i)} \\mathbf{y}_{(i)} \\\\ \\text { with } \\alpha_{(t, i)} &=\\frac{\\exp \\left(e_{t(i, i)}\\right)}{\\sum_{i} \\exp \\left(e_{(t ; i)}\\right)} \\\\ \\text { and } e_{(t, i)} &=\\left\\{\\begin{array}{ll}{\\mathbf{h}_{(i)}^{T} \\mathbf{y}_{(i)}} & {\\text { dot }} \\\\ {\\mathbf{h}_{(i)}^{T} \\mathbf{W} \\mathbf{y}_{(i)}} & {\\text { general }} \\\\ {\\mathbf{v}^{T} \\tanh \\left(\\mathbf{W}\\left[\\mathbf{h}_{i n} ; \\mathbf{y}_{i 0}\\right]\\right)} & {\\text { concat }}\\end{array}\\right.\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHdzICxF3lEX"
   },
   "source": [
    "<center><img src=\"https://drive.google.com/uc?id=1oHDx7OvqcyafvWVASk7pDrJDPrYvZsHN\" >\n",
    "\n",
    "[source: Lecture 8:\n",
    "Machine Translation,\n",
    "Sequence-to-sequence and Attention\n",
    "Abigail See](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf)  \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UGBluXLO10sG16Wr3k3bU4JMBOw9-Y5z\" >\n",
    "\n",
    "[source: Lecture 8:\n",
    "Machine Translation,\n",
    "Sequence-to-sequence and Attention\n",
    "Abigail See](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture08-nmt.pdf) \n",
    "\n",
    "</center>\n",
    "\n",
    "- To overcome the context bottleneck we will pass much more information: *all* of the hidden states from the input sequence, not just the last\n",
    "-  Attention uses queriable memory similar to a cross between a Map (Python dict) and a search tree\n",
    "- We can query using a key and get a weighted average of matching values, weighted by how well the keys match the query\n",
    "- Keys, values and queries are all tensors\n",
    "- So, rather than trying to keep everything in a single context vector for the sentence we will use a tensor\n",
    "- The longer the sentence, the bigger the tensor (usually one column per word in the sentence), which solves the capacity and gradient flow problem\n",
    "- The embeddings of each word are typically a concatenation of the hidden states of a forward and reverse RNN at that position in the sentence\n",
    "- Recurrent Attention: keys = values = input sequence; query is the hidden state from the previous step; calculates attention at each step\n",
    "- Word2vec doesn't seem to work as pretraining\n",
    "- Self-Attention: drop the recurrent connection; keys and values stay the same, but query becomes the current input\n",
    "- Learned Self-Attention: something the model learns during training\n",
    "- If we allow keys and values to be learned we move into a new area: Memory Networks e.g. Neural Turing Machines\n",
    "- The attention result is a sum of the weights of the words\n",
    "- The weights are a softmax of the similarity of each key and query\n",
    "- Two common methods of similarity\n",
    "  - Additive Similarity: Single layer neural network\n",
    "  - Multiplicative Similarity: Essentially cosine similarity (faster)\n",
    "- Multi-Head Attention: we can use several specialized attention mechanisms\n",
    "  - Semantic\n",
    "  - Grammatical\n",
    "  - Tense\n",
    "  - Context\n",
    "- Putting syntax knowledge back in may take us to a new level again\n",
    "- Typically still use LSTM but The Transformer doesn't (next) doesn't use RNN's at all\n",
    "- [Attention Visualization](https://distill.pub/2016/augmented-rnns/)\n",
    "- You can find the example of the implementation [here](https://www.tensorflow.org/tutorials/text/nmt_with_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AWBMw-b3lEX"
   },
   "source": [
    "## Multi-head Attention\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1Ft6U3JJZdII3PYdBKByj0eYFUB0J8hrO\" >\n",
    "\n",
    "[source: Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "</center>\n",
    "\n",
    "$$\\text { Attention }(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{T}}{\\sqrt{d_{keys}}}\\right) \\mathbf{v}$$\n",
    "\n",
    "$Q$ is a matrix containing one row per query. Its shape is [nqueries, dkeys], where nqueries is the number of queries and dkeys is the number of dimensions of each query and each key.\n",
    "\n",
    "$K$ is a matrix containing one row per key. Its shape is [nkeys, dkeys], where nkeys is the number of keys and values.\n",
    "\n",
    "$V$ is a matrix containing one row per value. Its shape is [nkeys, dvalues], where dvalues is the number of each value.\n",
    "\n",
    "The shape of ${Q K}^{T}$ is [nqueries, nkeys]: it contains one similarity score for each query/key pair. The output of the softmax function has the same shape, but all rows sum up to 1. The final output has a shape of [nqueries, dvalues]: there is one row per query, where each row represents the query result (a weighted sum of the values).\n",
    "\n",
    "The scaling factor scales down the similarity scores to avoid saturating the softmax function, which would lead to tiny gradients.\n",
    "\n",
    "It is possible to mask out some key/value pairs by adding a very large negative value to the corresponding similarity scores, just before computing the softmax. This is useful in the Masked Multi-Head Attention layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gygmWcCg3lEX"
   },
   "source": [
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1zOfgl5mxWzoKjlmb6r8AfIKltaVbhYUq\" >\n",
    "\n",
    "[source:Transformers and Self-Attention For Generative Models\n",
    "(guest lecture by Ashish Vaswani and Anna Huang)](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uoIHQ5a3lEY"
   },
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "- In 2017 Google showed that attention alone could outperform RNN's at translation (English to French and German)\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1tOF8_zc9jUFCIii8e8ZbnEtmduwhS3os\" >\n",
    "\n",
    "[source: Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "\n",
    "</center>\n",
    "\n",
    "Following image, you can observe we stack few encoders and decoders on top of each other to build a transformer.\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1qUERxT4gmi3BfYBn3UkO2_imdJOvm-P6\" ></center>\n",
    "\n",
    "\n",
    "- Whereas RNN's must proceed step-by-step (word by word), the Transformer uses only a small (configurable) number of steps\n",
    "- At each step it applies a self-attention mechanism to weigh the relative strength of the relationships the other words in the sentence have to the word being processed\n",
    "- A weighted average of all of the current words' representations are used to produce an updated representation for the target word\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=14Cw_bOYanC6O9gZO2MOs8jZ_j9YfVV3g\" ></center>\n",
    "\n",
    "- The translated sentence is generated a word at a time\n",
    "- Each decoded word attends to the representations of words already translated in the sentence as well as the representations of all of the words in the sentence being translated\n",
    "- The Transformer performs well on coreference resolution: where for example a pronoun could refer to either of two nouns earlier in the sentence\n",
    "- You can find the example of the implementation [here](https://www.tensorflow.org/tutorials/text/transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBCri_Yy3lEY"
   },
   "source": [
    "### Positional Embeddings \n",
    "\n",
    "A positional embedding is a dense vector which encodes the position of a word within a sentence. These embeddings are fixed, defined using the sine and cosine functions of different frequencies. This solution gives the same performance as learned positional embeddings do, but it can extend to arbitrarily long sentences, which is why it is favored. \n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1BF3df4rbKnDKbWkND-gO3_qHUtVG6eff\" >\n",
    "\n",
    "\n",
    "[source:Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition by Aurélien Chapter 16](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch16.html#)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29kxvMBf3lEY"
   },
   "source": [
    "### BERT\n",
    "\n",
    "BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.[source: Bert Github page](https://github.com/google-research/bert)\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1o6mwx4R345N4CV4werQq_i8VqGWAgU1t\" >\n",
    "\n",
    "[source BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UK-ockMovwV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HThu9dq40EZ-"
   },
   "source": [
    "#### Example: Classify text with BERT\n",
    "This notebook contains an example on how to fine-tune BERT to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX84gHqG0TW7"
   },
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H1yuNZWAzMPB",
    "outputId": "421eb4f9-8571-4e68-acae-6b570500c993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 4.3 MB 8.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 8.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 352 kB 50.3 MB/s \n",
      "\u001b[K     |████████████████████████████████| 636 kB 54.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 57.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 679 kB 34.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 211 kB 50.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 90 kB 7.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 99 kB 7.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 41 kB/s \n",
      "\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n",
      "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q -U tensorflow-text -q  # A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q tf-models-official -q   #AdamW optimizer from tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3wG52tO0r1U"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3WDKZFK1Kat"
   },
   "source": [
    "##### Data \n",
    "Let's use the plain-text IMDB movie reviews (i.e. Large Movie Review Dataset) which contains the text of 50,000 movie reviews from the Internet Movie Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJ32ZHpW0sEF",
    "outputId": "52a9d648-2458-4dc7-96ae-46e64bf174ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 1s 0us/step\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Download and extract the IMDB dataset\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "# Explore the directory structure\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "#Let's use the dataset from directory utility to create a labeled tf.data.Dataset\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2, #80:20 split of the training data \n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Let's create a validation set using an 80:20 split of the training data  \n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvQ6EK3p0sAp",
    "outputId": "6059af33-38d8-4bf7-b5e0-5df78c9abac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
      "Label : 0 (neg)\n",
      "Review: b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
      "Label : 0 (neg)\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at a few reviews\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(2):\n",
    "    print(f'Review: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qb0MHNs3Byn"
   },
   "source": [
    "##### Loading models from TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_ob-L8B0sKx",
    "outputId": "21f524d1-45f6-4437-8384-edd5877892a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "# Load the  a BERT model to fine-tune\n",
    "# here we use BERT-Base with fewer parameters (Uncased) which was released by the original BERT authors\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bpmxS9937hB"
   },
   "source": [
    "##### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_PPFeYf0sPo"
   },
   "outputs": [],
   "source": [
    "# load the preprocessing model into a hub.KerasLayer to compose the fine-tuned model\n",
    "# with the smallBert The input is truncated to 128 tokens (The number of tokens can be customized)\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kFZddzC0sTE",
    "outputId": "11ccfb77-0f82-4f11-c1cb-ffc7aae55094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [  101  1996  2526 15803  4767  2003  2107  2019  6429  3185   999   102]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Let's try the preprocessing model on some text\n",
    "text_test = [' The 2002 Bourne Identity is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n",
    "\n",
    "#Note: input_type_ids only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1SkIWVL5OW5"
   },
   "source": [
    "##### Using the BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrFH6DRl0sWr"
   },
   "outputs": [],
   "source": [
    "# load the model from TF HUB\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dwPOlxl0shY",
    "outputId": "8679d347-eb2c-48c6-9d0f-873e9d8d099e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.9599421   0.8852585  -0.13827781  0.49501595  0.23172547]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-1.249977    0.65426844  0.6727354  ... -0.4236857   0.2020679\n",
      "   0.00666368]\n",
      " [-0.7421359   0.02141126 -0.49172333 ... -1.0028691   0.05149584\n",
      "   0.19523036]\n",
      " [-1.2351371   0.37037274 -0.4866422  ... -0.66944957  0.19172102\n",
      "   0.06887599]\n",
      " [-1.3988634  -0.00379672 -0.4742977  ... -0.6985862   0.9620378\n",
      "   0.2523336 ]\n",
      " [-0.6657643   1.5479449   1.0051894  ... -0.59739363  0.3769249\n",
      "  -0.8154006 ]]\n"
     ]
    }
   ],
   "source": [
    "# Let's see the returned values using the example above\n",
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "# \"pooled_output\" : represents each input sequence as a whole. The shape is [batch_size, H]  -   an embedding for the entire movie review.\n",
    "# \"sequence_output\": represents each input token in the context. The shape is [batch_size, seq_length, H] -  a contextual embedding for every token in the movie review.\n",
    "# \"encoder_outputs\": are the intermediate activations of the L Transformer blocks. \n",
    "   # outputs[\"encoder_outputs\"][i] is a Tensor of shape [batch_size, seq_length, 1024] with the outputs of the i-th Transformer block, for 0 <= i < L. \n",
    "   # The last value of the list is equal to sequence_output.\n",
    "\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :5]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a18_v4lF6oCH"
   },
   "source": [
    "##### Define the classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vOKV_pJ0slQ"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kqgRb7RK0spK",
    "outputId": "6e03466e-fb5e-4821-8478-774241bec518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT raw results \n",
      " tf.Tensor([[0.37239307]], shape=(1, 1), dtype=float32)\n",
      "\n",
      "\n",
      "The model's structure \n",
      " <IPython.core.display.Image object>\n"
     ]
    }
   ],
   "source": [
    "# Let's check that the model runs with the output of the preprocessing model\n",
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(\"BERT raw results\", \"\\n\", tf.sigmoid(bert_raw_result))\n",
    "print(\"\\n\")\n",
    "print(\"The model's structure\", \"\\n\", tf.keras.utils.plot_model(classifier_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5z023lf7aHF"
   },
   "source": [
    "##### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roXxIiZW0sxL"
   },
   "outputs": [],
   "source": [
    "# Loss function: \n",
    "# Let's use losses.BinaryCrossentropy because we are doing a binary classification \n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViCDCA850s1q"
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6rNUD7M76EN"
   },
   "outputs": [],
   "source": [
    "# Loading the BERT model and training\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itB57G5F8Bxi",
    "outputId": "d9710a41-85d8-4f2f-bc06-d8e45a518b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/5\n",
      "605/625 [============================>.] - ETA: 2:44 - loss: 0.4852 - binary_accuracy: 0.7445"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9CFEYmg8Hj4"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss over time\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUFuXkkxKov8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7i9dV5r3lEY"
   },
   "source": [
    "# Resources\n",
    "\n",
    "- Contextual Word Representations: A Contextual Introduction by Noah A. Smith. arXiv:1902.06006v1\n",
    "- https://en.wikipedia.org/wiki/Word_embedding\n",
    "- https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
    "- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/\n",
    "- https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n",
    "- https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning\n",
    "- https://www.kaggle.com/drscarlat/imdb-sentiment-analysis-keras-and-tensorflow\n",
    "- https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "- https://medium.com/cityai/deep-learning-for-natural-language-processing-part-ii-8b2b99b3fa1e\n",
    "- https://www.tensorflow.org/guide/summaries_and_tensorboard\n",
    "- https://skymind.ai/wiki/attention-mechanism-memory-network\n",
    "- https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n",
    "- A Neural Attention Model for Abstractive Sentence Summarization: https://arxiv.org/abs/1509.00685\n",
    "- https://fasttext.cc/\n",
    "- https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
    "- https://www.tensorflow.org/text/guide/word_embeddings\n",
    "- https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ix7OJt6G3lEY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TBCri_Yy3lEY"
   ],
   "name": "07- Deep Models for Text (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0191caae8a16458e9042c869550e3db1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0486eae28e0e49fb9562fb88565939ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "063055bb350444b1a6aadcc2a5611ec0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "074b8cce066a49dd8b5c1e023de21dff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0898b64130d24b53ad4b464b967e42b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a29d599f6b94469a2d65f606f80fa0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0ec297f8167f4be7b999b8ef41c5162a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fa76a3de5a04d8182b34d247b7b7b70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ff6e7261fe848b182b9beab7cd28a96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30b0320d755d42e2b7c51640af912617",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c436a10513b74acd9cb7c1cc5aacc041",
      "value": 1
     }
    },
    "12d1925600a0404ba43358b8ebda2822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ec297f8167f4be7b999b8ef41c5162a",
      "placeholder": "​",
      "style": "IPY_MODEL_61506a8a75d94c8283e6d84451208ddb",
      "value": " 80/80 [00:03&lt;00:00, 24.30 MiB/s]"
     }
    },
    "133615349f36491195a3ba73aa5753a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "139cd977ff434d428cd5524e7a97f52e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d52cca5686f46d7b5111b5ceab2c921",
       "IPY_MODEL_4ea5040984ce46609940b229d0ac8f84"
      ],
      "layout": "IPY_MODEL_ab1283224d2c4f619f4c1a63f85313ae"
     }
    },
    "1613e50847a24c649d5154cdb4f80946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fa76a3de5a04d8182b34d247b7b7b70",
      "placeholder": "​",
      "style": "IPY_MODEL_c965b54a4c8d4ed0bf3a5ef8c0e18b04",
      "value": " 25000/0 [00:15&lt;00:00, 2887.18 examples/s]"
     }
    },
    "1d52cca5686f46d7b5111b5ceab2c921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": " 26%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc8d749de2dd4f079762ce03a43acaae",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0486eae28e0e49fb9562fb88565939ba",
      "value": 6485
     }
    },
    "22474f3161f044efa5ab0543c3530c48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f82dedde52ca4f538db8db64c5787baf",
      "placeholder": "​",
      "style": "IPY_MODEL_ebb6bb23b4534ffcaa34d3d90451a042",
      "value": " 40888/50000 [00:00&lt;07:47, 19.50 examples/s]"
     }
    },
    "29c4fcf911a544ecb5d989da4d23ea37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2adc2fbe57aa443f90a707efc60a34d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30146e76e00d4276828897f27f556ce8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6242ad4637aa44f0bd80f754784e07eb",
       "IPY_MODEL_7cf3d189d10544ca90954358f315b6b6"
      ],
      "layout": "IPY_MODEL_3da731aa490146f489fdea7d2435d789"
     }
    },
    "30b0320d755d42e2b7c51640af912617": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "334c6cdf0c5644e1974ad8f15674175b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "375d7fdc75a84c29b990c1bc3ffd81a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3bd1723cac4a14ae3bd9f49857e9e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3da731aa490146f489fdea7d2435d789": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42df465e1d7f4026af66e244f3ae410b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46fc795543054d9f8d9e58ded25cc371": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ea5040984ce46609940b229d0ac8f84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42df465e1d7f4026af66e244f3ae410b",
      "placeholder": "​",
      "style": "IPY_MODEL_cc09ed776ad842c7990a27ee36aaa1fe",
      "value": " 6485/25000 [00:00&lt;00:00, 64848.82 examples/s]"
     }
    },
    "5150d5e1538647dc9a673d567fef2a72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52f64d67cc2e41ffaae55268cc544c0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5d8796060074b3698f235a9d5e7208c",
      "placeholder": "​",
      "style": "IPY_MODEL_3c3bd1723cac4a14ae3bd9f49857e9e8",
      "value": " 24881/25000 [00:00&lt;00:00, 60643.47 examples/s]"
     }
    },
    "540308e19aee43b58bab940326e65a36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a2a0e12c3a6401f9f20cffb3170f376": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aeb95f3c078404f9d2bc34c1aa9524d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61506a8a75d94c8283e6d84451208ddb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6242ad4637aa44f0bd80f754784e07eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Completed...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_074b8cce066a49dd8b5c1e023de21dff",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3c9697bef2a464f8f955fafcfff7838",
      "value": 1
     }
    },
    "665392b42eab42b8a7b2934916179157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68f4ca31235c48ce851c6bade3a40f02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a7d0754efee4988aaa1e1225c0eea18",
       "IPY_MODEL_22474f3161f044efa5ab0543c3530c48"
      ],
      "layout": "IPY_MODEL_063055bb350444b1a6aadcc2a5611ec0"
     }
    },
    "7cf3d189d10544ca90954358f315b6b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a2a0e12c3a6401f9f20cffb3170f376",
      "placeholder": "​",
      "style": "IPY_MODEL_8c80fbcb2c5a49e2b8b73d730fcad011",
      "value": " 1/1 [00:03&lt;00:00,  3.35s/ url]"
     }
    },
    "8304d7c355334d8f99fe7eec48292ba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_133615349f36491195a3ba73aa5753a4",
      "placeholder": "​",
      "style": "IPY_MODEL_665392b42eab42b8a7b2934916179157",
      "value": " 25000/0 [00:16&lt;00:00, 2894.66 examples/s]"
     }
    },
    "837c1387e7cd4090931e8588018a636f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a0a21d201250419db610bf33db0a1c19",
       "IPY_MODEL_12d1925600a0404ba43358b8ebda2822"
      ],
      "layout": "IPY_MODEL_5150d5e1538647dc9a673d567fef2a72"
     }
    },
    "871887f1ef834d3dbe8da6a2ad14e009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_375d7fdc75a84c29b990c1bc3ffd81a4",
      "max": 25000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af8130d4b659468b8997cd84ded28f25",
      "value": 24881
     }
    },
    "8c80fbcb2c5a49e2b8b73d730fcad011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d99d12840b04adf81e370ae15d853a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9649a0a1c5b4ad8b8a6cd465730cff0",
       "IPY_MODEL_8304d7c355334d8f99fe7eec48292ba5"
      ],
      "layout": "IPY_MODEL_334c6cdf0c5644e1974ad8f15674175b"
     }
    },
    "8e3bcacfe0b2455c9ddf33104313732c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "938790171f544b099e0ad712a0e4d0d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ff6e7261fe848b182b9beab7cd28a96",
       "IPY_MODEL_1613e50847a24c649d5154cdb4f80946"
      ],
      "layout": "IPY_MODEL_46fc795543054d9f8d9e58ded25cc371"
     }
    },
    "9801932150934306bb27e6a89204c246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_871887f1ef834d3dbe8da6a2ad14e009",
       "IPY_MODEL_52f64d67cc2e41ffaae55268cc544c0b"
      ],
      "layout": "IPY_MODEL_5aeb95f3c078404f9d2bc34c1aa9524d"
     }
    },
    "9a7d0754efee4988aaa1e1225c0eea18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": " 82%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2adc2fbe57aa443f90a707efc60a34d8",
      "max": 50000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b6836df9d1c24f9aaea79cbf214f34d5",
      "value": 40888
     }
    },
    "a0a21d201250419db610bf33db0a1c19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Size...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_540308e19aee43b58bab940326e65a36",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a29d599f6b94469a2d65f606f80fa0d",
      "value": 1
     }
    },
    "ab1283224d2c4f619f4c1a63f85313ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aedbe3594e65427ead4b6b2cce898da8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8130d4b659468b8997cd84ded28f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "afc6a92d763942488ed2c96d4668052c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0898b64130d24b53ad4b464b967e42b7",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0191caae8a16458e9042c869550e3db1",
      "value": 1
     }
    },
    "b554d6348d484d098345a7bb6f812cc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aedbe3594e65427ead4b6b2cce898da8",
      "placeholder": "​",
      "style": "IPY_MODEL_29c4fcf911a544ecb5d989da4d23ea37",
      "value": " 50000/0 [00:22&lt;00:00, 3000.29 examples/s]"
     }
    },
    "b6836df9d1c24f9aaea79cbf214f34d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bc8d749de2dd4f079762ce03a43acaae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1af9a3c56274c56aa65fe0860a09bf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c436a10513b74acd9cb7c1cc5aacc041": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c5d8796060074b3698f235a9d5e7208c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c965b54a4c8d4ed0bf3a5ef8c0e18b04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc09ed776ad842c7990a27ee36aaa1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d317ae17625c484c878284c466ff59e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d936e45a4b1842529e1b9c26284630b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afc6a92d763942488ed2c96d4668052c",
       "IPY_MODEL_b554d6348d484d098345a7bb6f812cc8"
      ],
      "layout": "IPY_MODEL_c1af9a3c56274c56aa65fe0860a09bf9"
     }
    },
    "d9649a0a1c5b4ad8b8a6cd465730cff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e3bcacfe0b2455c9ddf33104313732c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d317ae17625c484c878284c466ff59e1",
      "value": 1
     }
    },
    "ebb6bb23b4534ffcaa34d3d90451a042": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3c9697bef2a464f8f955fafcfff7838": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f82dedde52ca4f538db8db64c5787baf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
